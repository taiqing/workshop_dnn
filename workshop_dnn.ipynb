{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Have a Taste of Deep Neural Network on the Shoulder of Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image](fruit_blob.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents\n",
    "<ul id=\"toc\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highlights of Deep Neural Networks\n",
    "\n",
    "Deep Neural Networks (DNNs) have achieved state-of-the-art results on a variety of tasks in different application fields.\n",
    "\n",
    "#### Image recognition\n",
    "* Face recognition\n",
    "    * Surpassed human-level performance on the LFW face dataset\n",
    "    \n",
    "* Object recognition\n",
    "    * More than 10,000 categories of images in the imageNet dataset\n",
    "\n",
    "#### Speech recognition\n",
    "* All major commercial speech recognition systems, e.g., \n",
    "\n",
    "    * Microsoft Cortana, \n",
    "    * Xbox, \n",
    "    * Skype Translator,\n",
    "    * Google Now, \n",
    "    * Apple Siri, *\n",
    "    * Baidu and *\n",
    "    * iFlyTek voice search\n",
    "     are based on deep learning methods.\n",
    "\n",
    "#### Natural Language Processing\n",
    "* Constituency parsing\n",
    "* Sentiment analysis\n",
    "* Information retrieval\n",
    "* Machine translation\n",
    "* Contextual entity linking and other areas of NLP.\n",
    "\n",
    "** Masters in the deep learning movement **\n",
    "![image](masters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Theano\n",
    "Theano is CPU/GPU math expression compiler, offered in the form of Python library.\n",
    "\n",
    "* Efficient handling of matrices\n",
    "* Efficient symbolic differentiation\n",
    "* Tight integration of NumPy\n",
    "* Dynamic C code generate\n",
    "* Transparent use of GPU\n",
    "\n",
    "Theano is developed by Y. Bengio and his colleagues.\n",
    "\n",
    "#### Theano's Philosophy - Fast to develop and Fast to run\n",
    "![image](fast.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine learning libraries built on top of Theano:\n",
    "* Pylearn2\n",
    "    * great flexibility and a good choice for trying out ML ideas\n",
    "* PyMC3\n",
    "    * Probabilistic programming; building statistical Bayesian models\n",
    "* Sklearn-theano\n",
    "    * Easy-to-use deep learning tool\n",
    "* Lasagne\n",
    "    * Lightweight library to build neural networks\n",
    "\n",
    "#### Models that have been built with Theano:\n",
    "* Deep Belief Networs (DBNs)\n",
    "* Convolutional Neural Networks (CNNs)\n",
    "* Recurrent Neural Networks (RNNs)\n",
    "* Long Short Term Memory (LSTM)\n",
    "* Autoencoders\n",
    "* GoogLeNet\n",
    "    * Winner of classification and object recognition challenges in ImageNet 2014 Challenge\n",
    "    * GoogLeNet presentation: http://image-net.org/challenges/LSVRC/2014/slides/GoogLeNet.pptx\n",
    "* Auto-generation of artistic image\n",
    "    * arXiv paper: http://arxiv.org/pdf/1508.06576v2.pdf\n",
    "\n",
    "![image](artistic_image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symbolic variables (tensor) in Theano\n",
    "* Variable (C, Java, Python, etc.)\n",
    "    * A segment of physical storage in RAM\n",
    "    * Operations are based on value passing between variables\n",
    "* Tensor (Theano)\n",
    "    * A mathematical symbol\n",
    "    * No physical storage in RAM to hold its value\n",
    "    * Operations are actually building connections between tensors\n",
    "* Shared variable (Theano)\n",
    "    * Hybrid of variable and tensor\n",
    "    * Tensor with physical storage in RAM to hold its value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "x = T.dvector(name='x')\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "y = f(x)\n",
    "\n",
    "theano.printing.pydotprint(theano.function([x], y), '1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Theano.function ** brings life to theano variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "\n",
    "x = T.dvector(name='x')\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "y = f(x)\n",
    "\n",
    "pow2 = theano.function(inputs=[x], outputs=y)\n",
    "\n",
    "a = np.array([1,2,3], dtype=theano.config.floatX)\n",
    "b = pow2(a)\n",
    "print \"a is {a}, b is {b}\".format(a=a, b=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "\n",
    "x = T.dvector('x')\n",
    "y = x.sum()\n",
    "grad = T.grad(cost=y, wrt=[x])\n",
    "grad_func = theano.function(inputs=[x], outputs=grad)\n",
    "\n",
    "a = np.array([1,2,3], dtype=theano.config.floatX)\n",
    "print grad_func(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "In the softmax regression setting, we are interested in multi-class classification. Suppose we have $m$ samples in the training set $\\{(x^{(1)}, y^{(1)}),...,(x^{(m)}, y^{(m)})\\}$, where $y^{(i)}\\in \\{1,2,...,k\\}$ and $x^{(i)}\\in R^{n}$.\n",
    "\n",
    "Given a test sample $x$, we want to estimate the probability that $x$ belongs to class $j$, i.e., $p(y=j|x)$, for all possible $j$.\n",
    "\n",
    "\\begin{equation}\n",
    "h_{W,b}(x) = \n",
    "\\left[\n",
    "  \\begin{array}{c}\n",
    "  p(y=1|x;W,b)\\\\\n",
    "  p(y=2|x;W,b)\\\\\n",
    "  ...\\\\\n",
    "  p(y=k|x;W,b)\\\\\n",
    "  \\end{array}\n",
    "\\right]\n",
    "=\\frac{1}{\\sum_{j=1}^{k}{e^{w_j^Tx+b_j}}}\n",
    "\\left[\n",
    "  \\begin{array}{c}\n",
    "  e^{w_1^Tx+b_1}\\\\\n",
    "  e^{w_2^Tx+b_2}\\\\\n",
    "  ...\\\\\n",
    "  e^{w_k^Tx+b_k}\\\\\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{equation}\n",
    "\n",
    "It is usually convenient to represent $W$ as a $n$-by-$k$ matrix, so that\n",
    "\\begin{equation}\n",
    "W = \\left[\\begin{array}{c}\n",
    "w_1^T\\\\\n",
    "...\\\\\n",
    "w_k^T\n",
    "\\end{array}\\right]\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Defining a Loss Function\n",
    "The loss of $h_{W, b}$ on the training set is\n",
    "\\begin{equation}\n",
    "J(W, b) = -\\frac{1}{m}\\left[\\sum_{i=1}^{m}\\sum_{j=1}^{k}1\\{y^{(i)}=j\\}\\log\\frac{e^{w_j^Tx^{(i)}+b_j}}{\\sum_{l=1}^{k}e^{w_l^Tx^{(i)}+b_j}}\\right] + \\frac{\\lambda}{2}\\sum_{i=1}^{k}\\sum_{j=1}^{n}w_{ij}^2\n",
    "\\end{equation}\n",
    "\n",
    "The first term is the negative log-likelihood of the training set, i.e., $-\\frac{1}{m}\\sum_{i=1}^{m}\\log{p(y^{(i)}|x^{(i)})}$.\n",
    "\n",
    "The second term is a weight decay term which penalizes large values of the parameters.\n",
    "\n",
    "$J(W,b)$ is a convex function, and thus gradient descent will not run into a local optima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the Model\n",
    "\n",
    "Gradient descent\n",
    "\n",
    "$$W \\leftarrow W - \\alpha \\frac{\\partial{J(W,b)}}{\\partial{W}}$$\n",
    "\n",
    "$$b \\leftarrow b - \\alpha \\frac{\\partial{J(W,b)}}{\\partial{b}}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "$$\\frac{\\partial{J(W,b)}}{\\partial{w_j}} = -\\frac{1}{m}\\sum_{i=1}^{m}\\left[x^{(i)}\\left(1\\{y^{(i)}\\} - p(y^{(i)}=j|x^{(i)})\\right)\\right] + \\lambda w_j$$\n",
    "\n",
    "$$\\frac{\\partial{J(W,b)}}{\\partial{b}} = -\\frac{1}{m}\\sum_{i=1}^{m}\\left(1\\{y^{(i)}\\} - p(y^{(i)}=j|x^{(i)})\\right)$$\n",
    "\n",
    "A more advanced alternative is the L-BFGS alogrithm, which also requires the gradient function as an input argument.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model with Theano\n",
    "\n",
    "#### Vectorising the model\n",
    "\n",
    "Notations\n",
    "* $X$: data matrix of size $n\\times m$, where $n$ is the number of dimensions and m the number of instances. That is, each column stores an instance.\n",
    "* $W$: weight matrix of size $k \\times n$, where $k$ is the number of classes.\n",
    "* $b$: bias vector of size $k\\times 1$.\n",
    "\n",
    "The posterior probability is\n",
    "\\begin{equation}\n",
    "p = softmax(WX+b)\n",
    "\\end{equation}\n",
    "where $softmax(M) = \\frac{M}{\\sum_{i}M_{ij}}$ and $M$ is an arbitrary matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities for manipulating the paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_size(shape):\n",
    "    \"\"\"\n",
    "    count the number of elements in a ndarray with shape=shape\n",
    "    \"\"\"\n",
    "    size = 1\n",
    "    for i in shape:\n",
    "        size *= i\n",
    "    return size\n",
    "    \n",
    "def pack(param_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        param_list: list of ndarrays\n",
    "    Returns:\n",
    "        theta:  vector of shape (?,), flattened params\n",
    "        shapes: list of tuples, with each tuple storing the shape of each ndarray in param_list\n",
    "    \"\"\"\n",
    "    shapes = []\n",
    "    theta=None\n",
    "    for p in param_list:\n",
    "        size = p.size\n",
    "        p2 = p.reshape((size, )) \n",
    "        if theta is None:\n",
    "            theta = p2\n",
    "        else:\n",
    "            theta = np.hstack((theta, p2))\n",
    "        shapes += [p.shape] \n",
    "    return theta, shapes\n",
    "        \n",
    "def unpack(theta, shapes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        theta:  vector of shape (?,), flattened params\n",
    "        shapes: list of tuples, with each tuple storing the shape of each ndarray in param_list      \n",
    "    Returns:\n",
    "        param_list: list of ndarrays\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    params = []\n",
    "    for shape in shapes:\n",
    "        size = get_size(shape)\n",
    "        x = theta[i:i+size]\n",
    "        params += [x.reshape(shape)]\n",
    "        i += size\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the softmax regression class\n",
    "\n",
    "![image](softmax_regression.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import gzip\n",
    "import cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from param_util import pack, unpack\n",
    "\n",
    "# for debugging\n",
    "theano.config.optimizer=\"fast_run\"\n",
    "theano.config.exception_verbosity=\"high\"\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "    def __init__(self, n_in, n_out, L2_reg_coef, max_iter=100):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.L2_reg_coef = L2_reg_coef\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        self.W = theano.shared(\n",
    "            value=0.005 * np.random.randn(self.n_out, self.n_in),\n",
    "            name='W',\n",
    "            borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            value=np.zeros((self.n_out,), dtype=theano.config.floatX),\n",
    "            name='b',\n",
    "            borrow=True)\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "        # sample matrix, each column stores a sample\n",
    "        X = T.dmatrix('X')\n",
    "        # label\n",
    "        y = T.lvector('y')\n",
    "        # predict\n",
    "        linear_input = (T.dot(self.W, X).transpose() + self.b).transpose()\n",
    "        p_y_given_x = self.__softmax__(linear_input)\n",
    "        pred = T.argmax(p_y_given_x, axis=0)\n",
    "        # NLL: negative log-likelihood\n",
    "        nll = -T.mean(T.log(p_y_given_x[y, T.arange(0, y.shape[0])]))\n",
    "        # cost (loss)\n",
    "        cost = nll + self.L2_reg_coef * (self.W**2).sum()\n",
    "        # error rate\n",
    "        error = 1.0*T.sum(T.neq(pred, y))/X.shape[1]\n",
    "        \n",
    "        self.test_model = theano.function(\n",
    "            inputs=[X, y],\n",
    "            outputs=error)\n",
    "        self.run_model = theano.function(\n",
    "            inputs=[X],\n",
    "            outputs=pred)\n",
    "            \n",
    "        # compute gradient\n",
    "        grads = T.grad(cost, self.params)\n",
    "        params = [p.type() for p in self.params]\n",
    "        self.grad_func = theano.function(\n",
    "            inputs=params + [X, y],\n",
    "            outputs=grads,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "        self.cost_func = theano.function(\n",
    "            inputs=params + [X, y],\n",
    "            outputs=cost,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])   \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        init_theta, shapes = pack([self.W.get_value(), self.b.get_value()])\n",
    "        opt_theta, opt_cost, d = sp.optimize.fmin_l_bfgs_b(\n",
    "            func=self.__cost_and_grad__,\n",
    "            x0=init_theta,\n",
    "            fprime=None,\n",
    "            args=(shapes, X, y),\n",
    "            maxiter=self.max_iter,\n",
    "            iprint=1)\n",
    "        opt_param_list = unpack(opt_theta, shapes)\n",
    "        for shared_param, opt_param in zip(self.params, opt_param_list):\n",
    "             shared_param.set_value(opt_param)      \n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.run_model(X)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        return self.test_model(X, y)\n",
    "        \n",
    "    def __cost_and_grad__(self, theta, *args):\n",
    "        shapes = args[0]\n",
    "        X = args[1]\n",
    "        y = args[2]\n",
    "        param_list = unpack(theta, shapes)\n",
    "        cost =  self.cost_func(*(param_list + [X, y]))\n",
    "        grad = self.grad_func(*(param_list + [X, y]))\n",
    "        grad, _ = pack(grad)\n",
    "        return cost, grad        \n",
    "    \n",
    "    def __softmax__(self, M):\n",
    "        \"\"\"\n",
    "        normalise along the vertical axis\n",
    "        \"\"\"\n",
    "        e_M = T.exp(M - M.max(axis=0, keepdims=True))\n",
    "        return e_M / e_M.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model\n",
    "\n",
    "##### Dataset: MNIST Dataset of Handwritten Digits\n",
    "\n",
    "* Gray-scale image of size $28\\times 28$ with value ranging from [0, 1].\n",
    "* 50,000 training samples, 10,000 validation samples and 10,000 testing samples.\n",
    "\n",
    "![image](mnist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    plt.close('all')\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    f = gzip.open('data/mnist.pkl.gz')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    \n",
    "    # train model\n",
    "    train_X = train_set[0].transpose()\n",
    "    train_y = train_set[1]\n",
    "    valid_X = valid_set[0].transpose()\n",
    "    valid_y = valid_set[1]\n",
    "    \n",
    "    train_X = np.hstack((train_X, valid_X))\n",
    "    train_y = np.hstack((train_y, valid_y))\n",
    "    \n",
    "    train_X = train_X[:, 0:60001:5]\n",
    "    train_y = train_y[0:60001:5]\n",
    "    \n",
    "    n_in = train_X.shape[0]\n",
    "    n_out = np.unique(train_y).shape[0]\n",
    "    print \"{n} training samples of dim {d}\".format(n=train_X.shape[1], d=n_in)\n",
    "    \n",
    "    softmax_regression = SoftmaxRegression(n_in, n_out, L2_reg_coef=1e-4, max_iter=100)\n",
    "    softmax_regression.fit(train_X, train_y)\n",
    "    \n",
    "    # test model\n",
    "    test_X = test_set[0].transpose()\n",
    "    test_y = test_set[1]\n",
    "    error = softmax_regression.evaluate(test_X, test_y)\n",
    "    print 'error rate on test set is {e}%, accuracy is {a}%'.format(e=error*100, a=100-100*error)\n",
    "    # baseline method\n",
    "    pred_baseline = np.random.randint(\n",
    "        low = np.min(train_y),\n",
    "        high = np.max(train_y)+1,\n",
    "        size=valid_y.shape)\n",
    "    error_baseline = 1.0 * (pred_baseline != valid_y).sum() / valid_X.shape[1]\n",
    "    print 'baseline: error rate on test set is {e}%, accurary is {a}%'.format(e=error_baseline*100, a=100-100*error_baseline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the \"standard digits\" in the eyes of Softmax Regression?\n",
    "\n",
    "Find input $x$ that maximally activates the $k$-th output unit and has limited $L_2$ norm: \n",
    "$$\n",
    "\\max_{x}h_k(x)\\\\\n",
    "s.t. \\Vert x \\Vert^2 \\le 1\n",
    "$$\n",
    "This problem can be approximated by a non-constrained minimisation problem:\n",
    "$$\n",
    "\\min_{x}\\left[-h_k(x) + \\lambda \\max(\\Vert x \\Vert^2 - 1, 0)^2\\right]\n",
    "$$\n",
    "where $\\lambda$ is a fairly large number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visulisation\n",
    "plt.close('all')\n",
    "def rescale(x):\n",
    "    # Remove DC (mean of images)\n",
    "    x = x - x.mean()\n",
    "    # Truncate to +/-3 standard deviations and scale to -1 to 1\n",
    "    pstd = 3 * x.std()\n",
    "    x = np.maximum(np.minimum(x, pstd), -pstd) / pstd\n",
    "    # Rescale from [-1,1] to [0.1,0.9]\n",
    "    x = (x + 1) * 0.4 + 0.1;\n",
    "    return x \n",
    "def softmax(M):\n",
    "    e_M = T.exp(M - M.max(axis=0, keepdims=True))\n",
    "    return e_M / e_M.sum(axis=0, keepdims=True)\n",
    "W = softmax_regression.W.get_value()\n",
    "b = softmax_regression.b.get_value()\n",
    "fig = plt.figure(0)\n",
    "for i in np.arange(0, 10):\n",
    "    x = T.dvector('x')\n",
    "    x0 = np.random.rand(28*28)\n",
    "    x0 = x0 / np.sqrt((x0**2).sum())\n",
    "    p_y_given_x = softmax(T.dot(W, x) + b)\n",
    "    # cost\n",
    "    cost = -T.log(p_y_given_x[i])  + 1e2 * T.max([0, (x**2).sum()-1])**2\n",
    "    cost_func = theano.function([x], cost)\n",
    "    grad = T.grad(cost, x)\n",
    "    grad_func = theano.function([x], grad)\n",
    "    proba_func = theano.function([x], p_y_given_x)\n",
    "    def cost_and_grad(x):\n",
    "        return cost_func(x), grad_func(x)\n",
    "    opt_x, opt_cost, d = sp.optimize.fmin_l_bfgs_b(\n",
    "        func=cost_and_grad,\n",
    "        x0=x0,\n",
    "        maxiter=100,\n",
    "        iprint=0)\n",
    "    #print opt_x\n",
    "    print \"||x||^2 is {v:.2f}, proab is {p:.4f}\".format(v=(opt_x**2).sum(), p=proba_func(opt_x)[i])\n",
    "    opt_x_rescaled = rescale(opt_x)\n",
    "    img = opt_x_rescaled.reshape((28,28))\n",
    "    ax = fig.add_subplot(2, 5,i+1)\n",
    "    ax.imshow(img, cmap='gray', interpolation='none')\n",
    "    ax.set_axis_off()\n",
    "plt.show()\n",
    "fig.savefig('visual_softmax_regression.png', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](visual_softmax_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "#### Neuron: Building block of neural network\n",
    "\n",
    "![image](neuron_brain.jpg)\n",
    "![image](neuron.png)\n",
    "\n",
    "$$h_{W,b}(x) = f(W^Tx+b) = f(\\sum_{i=1}^{3}W_ix_i + b)$$\n",
    "\n",
    "where $f: \\mathcal{R}\\rightarrow\\mathcal{R}$ is called the activation function. \n",
    "\n",
    "Common choices for $f$ include the sigmoid function and the tanh function. We use the sigmoid function in this talk.\n",
    "\n",
    "sigmoid function:\n",
    "$$f(z) = \\frac{1}{1 + \\exp(-z)}$$\n",
    "\n",
    "tanh function:\n",
    "$$f(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$\n",
    "\n",
    "![image](sigmoid.png) ![image](tanh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network model\n",
    "\n",
    "![image](nn.png)\n",
    "\n",
    "##### Notations\n",
    "* $m$: the number of samples\n",
    "* $n_l$: the number of layers, including the input, hidden and output layers\n",
    "* $s_l$: the number of units in layer $L_l$, excluding the bias unit\n",
    "* $W^{(l)}$: the weight matrix connecting layer $L_l$ and layer $L_{l+1}$\n",
    "* $W_{ji}^{(l)}$: the weight connecting unit $i$ in layer $L_l$ and unit $j$ in layer $L_{l+1}$\n",
    "* $b^{(l)}$: the bias vector connecting the bias unit in layer $L_l$ to the units in layer $L_{l+1}$\n",
    "* $z^{(l)}_j$: the linear input of the unit $j$ in layer $L_l$\n",
    "* $a^{(l)}_j$: the activation of the unit $j$ in layer $L_l$\n",
    "\n",
    "![image](simple_nn.png)\n",
    "\n",
    "$$a_1^{(2)} = f(W_{11}^{(1)}x_1 + W_{12}^{(1)}x_1 + W_{13}^{(1)}x_1 + b_1^{(1)})$$\n",
    "\n",
    "$$a_2^{(2)} = f(W_{21}^{(1)}x_1 + W_{22}^{(1)}x_1 + W_{23}^{(1)}x_1 + b_1^{(2)})$$\n",
    "\n",
    "$$a_1^{(3)} = f(W_{31}^{(1)}x_1 + W_{32}^{(1)}x_1 + W_{33}^{(1)}x_1 + b_1^{(3)})$$\n",
    "\n",
    "$$h_{W,b}(x) = a_1^{(3)} = f(W_{11}^{(2)}a_1 + W_{12}^{(2)}a_1 + W_{13}^{(2)}a_1 + b_1^{(2)})$$\n",
    "\n",
    "If we extend the activation function $f(\\cdot)$ to apply to vectors in an element-wise fashion (i.e., $f([z_1, z_2, z_3]) = [f(z_1), f(z_2), f(z_3)])$, we can rewrite the equations above more compactly as\n",
    "\n",
    "$$z^{(2)} = W^{(1)}x + b^{(1)}$$\n",
    "\n",
    "$$a^{(2)} = f(z^{(2)})$$\n",
    "\n",
    "$$z^{(3)} = W^{(2)}a^{(2)} + b^{(2)}$$\n",
    "\n",
    "$$h_{W,b}(x) = a^{(3)} = f(z^{(3)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation - RUNNING THE MODEL\n",
    "\n",
    "Set $a^{(1)} = x$ to denote the values from the input layer. \n",
    "\n",
    "Given layer $L_l$'s activations $a^{(l)}$, we can compute layer $L_{l+1}$'s activations $a^{(l+1)}$ as\n",
    "\n",
    "$$z^{(l+1)} = W^{(l)}a^{(l)} + b^{(l)}$$\n",
    "\n",
    "$$a^{(l+1)} = f(z^{(l+1)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation - TRAINING THE MODEL\n",
    "\n",
    "Suppose we have a training set $\\{(x^{(1)}, y^{(1)}),...,(x^{(m)}, y^{(m)})\\}$. \n",
    "\n",
    "For a single training example $(x, y)$, the loss function is defined as\n",
    "\n",
    "$$J(W,b; x,y) = \\frac{1}{2}\\Vert h_{W,b}(x)-y\\Vert^2$$\n",
    "\n",
    "Given a training set of $m$ examples, the over loss function is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "J(W,b) &= \\left[\\frac{1}{m}\\sum_{i=1}^{m}J(W,b; x^{(i)}, y^{(i)})\\right] + \\frac{\\lambda}{2}\\sum_{l=1}^{n_l-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}\\left(W_{ji}^{(l)}\\right)^2\\\\\n",
    "&=\\left[\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\frac{1}{2}\\Vert h_{W,b}(x)-y\\Vert^2\\right)\\right] + \\frac{\\lambda}{2}\\sum_{l=1}^{n_l-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}\\left(W_{ji}^{(l)}\\right)^2\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The second term is a regularisation term (also known as weight decay term) that tends to decrease the magnitude of the weights and helps prevent overfitting.\n",
    "\n",
    "Our goal is to minimise $J(W,b)$ as a function of $W$ and $b$, which can be solved by various gradent descent based alogrithms.\n",
    "\n",
    "\\begin{equation}\n",
    "W_{ij}^{(l)} \\leftarrow W_{ij}^{(l)} - \\alpha \\frac{\\partial J(W,b)}{\\partial W_{ij}^{(l)}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "b_i^{(l)} \\leftarrow b_i^{(l)} - \\alpha \\frac{\\partial J(W,b)}{\\partial b_i^{(l)}}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(W,b)}{\\partial W_{ij}^{(l)}} = \\left[\\frac{1}{m}\\sum_{i=1}^{m}\\frac{\\partial}{\\partial W_{ij}^{(l)}}J(W,b;x^{(i)}, y^{(i)})\\right] + \\lambda W_{ij}^{(l)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(W,b)}{\\partial b_i^{(l)}} = \\frac{1}{m}\\sum_{i=1}^{m}\\frac{\\partial}{\\partial b_i^{(l)}}J(W,b;x^{(i)},y^{(i)})\n",
    "\\end{equation}\n",
    "\n",
    "Calculation of $\\frac{\\partial}{\\partial W_{ij}^{(l)}}J(W,b;x^{(i)}, y^{(i)})$:\n",
    "\n",
    "![image](nn_gradient_1.jpg)\n",
    "![image](nn_gradient_2.jpg)\n",
    "![image](nn_gradient_3.jpg)\n",
    "\n",
    "The backpropagation algorithm is\n",
    "\n",
    "1. Perform a forward propagation, computing the activations of each layer.\n",
    "\n",
    "2. For each output unit $i$ in the output layer $L_{n_l}$, set\n",
    "\n",
    "    \\begin{equation}\n",
    "    \\delta_i^{n_l} = \\frac{\\partial}{\\partial z_i^{(n-l)}}\\frac{1}{2}\\Vert y - h_{W,b}(x)\\Vert^2 = -(y_i-a_i^{(n_l)})\\cdot f^{'}(z_i^{n_l})\n",
    "    \\end{equation}\n",
    "\n",
    "    where we use $\\cdot$ to denote the element-wise product operator (also known as Hadamard product).\n",
    "\n",
    "3. For $l = n_l-1, n_l-2, ...,2$\n",
    "\n",
    "    For each node $i$ in layer $L_l$, set\n",
    "\n",
    "    \\begin{equation}\n",
    "    \\delta_i^{(l)} = \\left(\\sum_{j=1}^{s_{l+1}}W_{ji}^{(l)}\\delta_j^{(l+1)}\\right)f^{'}(z_i^{l})\n",
    "    \\end{equation}\n",
    " \n",
    "4. Compute the desired partial derivatives\n",
    "\n",
    "    \\begin{equation}\n",
    "    \\frac{\\partial}{\\partial W_{ij}^{(l)}}J(W,b;x,y) = a_j^{(l)}\\delta_i^{(l+1)}\n",
    "    \\end{equation}\n",
    "    \n",
    "    \\begin{equation}\n",
    "    \\frac{\\partial}{\\partial b_{i}^{(l)}}J(W,b;x,y) = \\delta_i^{(l+1)}\n",
    "    \\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sparse Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "Suppose we have only a set of unlabeled training examples $\\{x^{(1)},...,x^{(m)}\\}$, where $x^{(i)}\\in R^n$.\n",
    "\n",
    "An autoencoder neural network is an unsupervised learning algorithm that applies backpropagation, setting the target values to be equal to the inputs, i.e., $y^{(i)}=x^{(i)}$.\n",
    "\n",
    "![image](autoencoder.png)\n",
    "\n",
    "The autoencoder tries to learn an identity function $h_{W,b}(x)\\approx x$.\n",
    "* The identity function seems a particularly trivial function to be trying to learn;\n",
    "* By placing constraints on the network, such as by limiting the number of hidden units, we can discover interesting structure about the data.\n",
    "* As a concrete example, suppose the inputs $x$ are the pixel intensity values from a $10\\times 10$ image ($100$ pixels) so $n=100$, and there are $s_2=50$ hidden units in layer $L_2$. \n",
    "Since there are only $50$ hidden units, the network is forced to learn a compressed representation of the input.\n",
    "* If the inputs are completely random, i.e., each dimension comes from an independent distribution, the autoencoding task would be very difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Forward propagation\n",
    "\n",
    "![image](autoencoder_small.png)\n",
    "\n",
    "$$z^{(2)} = W^{(1)}x + b^{(1)}$$\n",
    "\n",
    "$$a^{(2)} = sigmoid(z^{(2)})$$\n",
    "\n",
    "$$z^{(3)} = W^{(2)}a^{(2)} + b^{(2)}$$\n",
    "\n",
    "$$a^{(3)} = sigmoid(z^{(3)})$$\n",
    "\n",
    "$$h_{W,b}(x) = a^{(3)}(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Loss Function\n",
    "\n",
    "For a single training sample $x$, the loss function is defined as\n",
    "\\begin{equation}\n",
    "J(W,b; x) = \\frac{1}{2}\\Vert h_{W,b}(x)-x\\Vert^2\n",
    "\\end{equation}\n",
    "Given a trainig set of $m$ samoples, we define the loss function as\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "J(W,b) &= \n",
    "\\left[\\frac{1}{m}\\sum_{i=1}^{m}J(W,b; x^{(i)})\\right]\n",
    "+\\lambda \\sum_{l=1}^{n_{l-1}}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}\\left(W_{ji}^{(l)}\\right)^2\\\\\n",
    "&=\\left[\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\frac{1}{2}\\Vert h_{W,}(x^{(i)})-x^{(i)}\\Vert^2\\right)\\right]\n",
    "+\\lambda \\sum_{l=1}^{n_{l-1}}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}\\left(W_{ji}^{(l)}\\right)^2\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The second term is a weight decay term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity Constraint\n",
    "\n",
    "We would like to constrain the neurons to be active only for a subset of patterns.\n",
    "* $a^{(2)}_j(x)$ denotes the activiation of hidden unit $j$ when the network is given a specific input $x$.\n",
    "* The average activation of hidden unit $j$ over the training set is\n",
    "\\begin{equation}\n",
    "\\hat{\\rho}_j = \\frac{1}{m}\\sum_{i=1}^{m}\\left[a^{(2)}_j(x^{(i)})\\right]\n",
    "\\end{equation}\n",
    "* We would like to enforce the constraint\n",
    "$$\\hat{\\rho}_j=\\rho$$\n",
    "where $\\rho$ is a sparsity parameter, typically a small value close to zero.\n",
    "* A penalty term that penalises $\\hat{\\rho}_j$ deviating significantly from $\\rho$\n",
    "\\begin{equation}\n",
    "\\sum_{j=1}^{s_2}KL(\\rho||\\hat{\\rho}_j) = \\sum_{j=1}^{s_2}\\left[\\rho\\log\\frac{\\rho}{\\hat{\\rho}_j} + (1-\\rho)\\log\\frac{1-\\rho}{1-\\hat{\\rho}_j}\\right]\n",
    "\\end{equation}\n",
    "    * $KL(\\rho||\\hat{\\rho}_j)$ has the property that $KL(\\rho||\\hat{\\rho}_j) = 0$ if $\\hat{\\rho}_j=\\rho$, and otherwise it increases monotonically as $\\hat{\\rho}_j$ diverges from $\\rho$.\n",
    "    * In the figure below, we set $\\rho = 0.2$.\n",
    "\n",
    "![image](kl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our overall loss function for sparse autoencoder is\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "J(W,b) &= \n",
    "\\left[\\frac{1}{m}\\sum_{i=1}^{m}J(W,b; x^{(i)})\\right]\n",
    "+\\lambda \\sum_{l=1}^{n_{l-1}}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}\\left(W_{ji}^{(l)}\\right)^2 + \\beta\\sum_{j=1}^{s_2}KL(\\rho||\\hat{\\rho}_j)\\\\\n",
    "&=\\left[\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\frac{1}{2}\\Vert h_{W,b}(x^{(i)})-x^{(i)}\\Vert^2\\right)\\right]\n",
    "+\\lambda \\sum_{l=1}^{n_{l-1}}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}\\left(W_{ji}^{(l)}\\right)^2 + \\beta \\sum_{j=1}^{s_2}\\left[\\rho\\log\\frac{\\rho}{\\hat{\\rho}_j} + (1-\\rho)\\log\\frac{1-\\rho}{1-\\hat{\\rho}_j}\\right]\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{\\rho}_j$ is\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\rho}_j = \\frac{1}{m}\\sum_{i=1}^{m}\\left[a^{(2)}_j(x^{(i)})\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model with Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities for paramter manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import numpy as np\n",
    "\n",
    "def get_size(shape):\n",
    "    \"\"\"\n",
    "    count the number of elements in a ndarray with shape=shape\n",
    "    \"\"\"\n",
    "    size = 1\n",
    "    for i in shape:\n",
    "        size *= i\n",
    "    return size\n",
    "    \n",
    "def pack(param_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        param_list: list of ndarrays\n",
    "    Returns:\n",
    "        theta:  vector of shape (?,), flattened params\n",
    "        shapes: list of tuples, with each tuple storing the shape of each ndarray in param_list\n",
    "    \"\"\"\n",
    "    shapes = []\n",
    "    theta=None\n",
    "    for p in param_list:\n",
    "        size = p.size\n",
    "        p2 = p.reshape((size, )) \n",
    "        if theta is None:\n",
    "            theta = p2\n",
    "        else:\n",
    "            theta = np.hstack((theta, p2))\n",
    "        shapes += [p.shape] \n",
    "    return theta, shapes\n",
    "        \n",
    "def unpack(theta, shapes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        theta:  vector of shape (?,), flattened params\n",
    "        shapes: list of tuples, with each tuple storing the shape of each ndarray in param_list      \n",
    "    Returns:\n",
    "        param_list: list of ndarrays\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    params = []\n",
    "    for shape in shapes:\n",
    "        size = get_size(shape)\n",
    "        x = theta[i:i+size]\n",
    "        params += [x.reshape(shape)]\n",
    "        i += size\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a single layer\n",
    "\n",
    "![image](softmax_regression.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, n_in, n_out, name=''):\n",
    "        r = -np.sqrt(6.0 / (n_in + n_out + 1))\n",
    "        rand_W = 2 * r * np.random.rand(n_out, n_in) - r\n",
    "        self.W = theano.shared(\n",
    "            value=rand_W,\n",
    "            name='W_'+name,\n",
    "            borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            value=np.zeros((n_out,), dtype=theano.config.floatX),\n",
    "            name='b_'+name,\n",
    "            borrow=True)\n",
    "        self.params = [self.W, self.b]\n",
    "        self.L2_sqr = (self.W ** 2).sum()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: matrix tensor, each column stores a sample\n",
    "        \"\"\"\n",
    "        linear_input = (T.dot(self.W, X).transpose() + self.b).transpose()\n",
    "        activation = T.nnet.sigmoid(linear_input)\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a AutoEncoder class\n",
    "\n",
    "![image](autoencoder_small.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import gzip\n",
    "import cPickle\n",
    "\n",
    "#from param_util import pack, unpack\n",
    "#from layer import Layer\n",
    "\n",
    "# for debugging\n",
    "theano.config.optimizer='fast_run'#'fast_compile'\n",
    "theano.config.exception_verbosity=\"high\"\n",
    "\n",
    "class AutoEncoder(object):\n",
    "    def __init__(self, n_in, n_hid, \n",
    "                 L2_reg_coef, sparse_reg_coef, sparse_rho, \n",
    "                 max_iter=400):\n",
    "        self.n_in = n_in\n",
    "        self.n_hid = n_hid\n",
    "        self.L2_reg_coef = L2_reg_coef\n",
    "        self.sparse_reg_coef = sparse_reg_coef\n",
    "        self.sparse_rho = sparse_rho\n",
    "        self.max_iter = max_iter \n",
    "        \n",
    "        self.hidden_layer = Layer(self.n_in, self.n_hid, 'hidden')\n",
    "        self.output_layer = Layer(self.n_hid, self.n_in, 'output')\n",
    "        self.params = self.hidden_layer.params + self.output_layer.params\n",
    "        \n",
    "        X = T.dmatrix(name='X') # each column stores a sample\n",
    "        m = X.shape[1] # sample count\n",
    "        activation = self.hidden_layer.forward(X)\n",
    "        output = self.output_layer.forward(activation)\n",
    "        error = ((output - X) ** 2).sum() / (2.0 * m)\n",
    "        L2_reg = self.hidden_layer.L2_sqr + self.output_layer.L2_sqr\n",
    "        rho = activation.mean(axis=1)\n",
    "        kl = (self.sparse_rho * T.log(self.sparse_rho/rho) + \n",
    "              (1-self.sparse_rho) * T.log((1-self.sparse_rho)/(1-rho))).sum()\n",
    "        cost = error + self.L2_reg_coef * 0.5 * L2_reg + self.sparse_reg_coef * kl\n",
    "        grads = T.grad(cost, self.params)\n",
    "        params = [p.type() for p in self.params]\n",
    "        self.cost_func = theano.function(\n",
    "            inputs=params + [X],\n",
    "            outputs=cost,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "        self.grad_func = theano.function(\n",
    "            inputs=params + [X],\n",
    "            outputs=grads,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "        self.run_model = theano.function(\n",
    "            inputs=[X],\n",
    "            outputs=activation)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: matrix tensor, each column stores a sample\n",
    "        \"\"\"\n",
    "        return self.hidden_layer.forward(X)\n",
    "            \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        X: np.ndarray, each column stores a sample\n",
    "        \"\"\"\n",
    "        init_theta, shapes = pack([\n",
    "            self.hidden_layer.W.get_value(), self.hidden_layer.b.get_value(), \n",
    "            self.output_layer.W.get_value(), self.output_layer.b.get_value()])\n",
    "        opt_theta, opt_cost, d = sp.optimize.fmin_l_bfgs_b(\n",
    "            func=self.cost_and_grad,\n",
    "            x0=init_theta,\n",
    "            fprime=None,\n",
    "            args=(shapes, X),\n",
    "            maxiter=self.max_iter,\n",
    "            iprint=1)\n",
    "        opt_param_list = unpack(opt_theta, shapes)\n",
    "        for shared_param, opt_param in zip(self.params, opt_param_list):\n",
    "             shared_param.set_value(opt_param)\n",
    "             \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        X: np.ndarray, each column stores a sample\n",
    "        \"\"\"\n",
    "        return self.run_model(X)\n",
    "\n",
    "    def cost_and_grad(self, theta, *args):\n",
    "        \"\"\"\n",
    "        compute the cost and gradient at theta in the parameter space\n",
    "        args should be (shapes, X)\n",
    "        X: each column stores a sample\n",
    "        \"\"\"\n",
    "        shapes = args[0]\n",
    "        X = args[1]\n",
    "        param_list = unpack(theta, shapes)\n",
    "        cost =  self.cost_func(*(param_list+[X]))\n",
    "        grad = self.grad_func(*(param_list+[X]))\n",
    "        grad, _ = pack(grad)\n",
    "        return cost, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    plt.close('all')\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    f = gzip.open('data/mnist.pkl.gz')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    train_X = train_set[0].transpose()\n",
    "    X = train_X[:, 0:50001:5]\n",
    "    print '{m} samples of dimension {d}'.format(m=X.shape[1], d=X.shape[0])\n",
    "    auto_encoder = AutoEncoder(\n",
    "        n_in=X.shape[0],\n",
    "        n_hid = 200,\n",
    "        L2_reg_coef=3e-3,\n",
    "        sparse_reg_coef=3.0,\n",
    "        sparse_rho=0.1,\n",
    "        max_iter=200)\n",
    "    auto_encoder.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising a Trained Autoencoder\n",
    "\n",
    "Given a specific input $x$, the activation of hidden unit $i$ is\n",
    "$$a^{(2)}_i(x) = sigmoid(\\sum_{j=1}^{s_1}W_{ij}^{(1)}x_j + b_i)$$\n",
    "\n",
    "![image](autoencoder_small.png)\n",
    "\n",
    "Find the input $x^*$ that maximises $a^{(2)}_i$, i.e., maximally activating hidden unit $i$.\n",
    "\n",
    "Considering\n",
    "* Sigmoid function is a monotonically increasing function;\n",
    "* The input should be constrained to have a limited magnitude,\n",
    "\n",
    "we have\n",
    "\\begin{equation}\n",
    "x^* = \\arg\\max_{x}{\\sum_{j=1}^{s_1}W_{ij}^{(1)}x_j + b_i}\\\\\n",
    "s.t. \\Vert x \\Vert^2 \\le 1\n",
    "\\end{equation}\n",
    "\n",
    "** Lemma 1** The optimal $x^*$ has identity norm, that is $\\Vert x^* \\Vert^2 = 1$.\n",
    "\n",
    "Solve for $x^*$ by lagrange multiplier:\n",
    "\\begin{equation}\n",
    "F(x, \\lambda) = {\\sum_{j=1}^{s_1}W_{ij}^{(1)}x_j + b_i} + \\lambda \\left(\\sum_{j=1}^{s_1}x_j^2-1\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Setting\n",
    "\\begin{equation}\n",
    "\\frac{\\partial F(x, \\lambda)}{\\partial x_j} = W_{ij}^{(1)}x_j + 2\\lambda x_j = 0\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial F(x, \\lambda)}{\\partial \\lambda} = \\sum_{j=1}^{s_1}x_j^2 - 1 = 0\n",
    "\\end{equation}\n",
    "\n",
    "By solving the above equations, we have\n",
    "\\begin{equation}\n",
    "x_j^* = \\frac{W_{ij}^{(1)}}{\\sqrt{\\sum_{j=1}^{s_1}\\left(W_{ij}^{(1)}\\right)^2}}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    fig = plt.figure(0)\n",
    "    n = 8#int(np.floor(np.sqrt(auto_encoder.n_hid)))\n",
    "    for i in np.arange(n**2):\n",
    "        ax = fig.add_subplot(n, n, i+1)\n",
    "        w = auto_encoder.hidden_layer.W.get_value()[i, :]\n",
    "        w = w / np.sqrt((w ** 2).sum())\n",
    "        w = (w - w.min()) / (w.max()-w.min())\n",
    "        d = np.floor(np.sqrt(w.shape[0]))\n",
    "        ax.imshow(w.reshape((d,d)), cmap='gray', interpolation='none')\n",
    "        ax.set_axis_off()\n",
    "    plt.show()\n",
    "    fig.savefig('visual_ae.png', format='png')\n",
    "    \n",
    "    # visulisation the first hidden layer\n",
    "    def rescale(x):\n",
    "        # Remove DC (mean of images)\n",
    "        x = x - x.mean()\n",
    "        # Truncate to +/-3 standard deviations and scale to -1 to 1\n",
    "        pstd = 3 * x.std()\n",
    "        x = np.maximum(np.minimum(x, pstd), -pstd) / pstd\n",
    "        # Rescale from [-1,1] to [0.1,0.9]\n",
    "        x = (x + 1) * 0.4 + 0.1;\n",
    "        return x\n",
    "    fig = plt.figure(0)\n",
    "    k = 0\n",
    "    for i in range(0, 25):#for i in np.random.randint(0, dnn.ae_layers[0].n_hid, 64):\n",
    "        x = T.dvector('x')\n",
    "        x0 = np.random.rand(28*28)\n",
    "        x0 = x0 / np.sqrt((x0**2).sum())\n",
    "        a = auto_encoder.forward(x)\n",
    "        # cost\n",
    "        cost = -a[i] + 1e2 * T.max([0, (x**2).sum()-1])**2\n",
    "        grad = T.grad(cost, x)\n",
    "        cost_func = theano.function([x], cost)\n",
    "        grad = T.grad(cost, x)\n",
    "        grad_func = theano.function([x], grad)\n",
    "        act_func = theano.function([x], a)\n",
    "        def cost_and_grad(x):\n",
    "            return cost_func(x), grad_func(x)\n",
    "        opt_x, opt_cost, d = sp.optimize.fmin_l_bfgs_b(\n",
    "            func=cost_and_grad,\n",
    "            x0=x0,\n",
    "            maxiter=1000,\n",
    "            iprint=0)\n",
    "        opt_x_rescaled = rescale(opt_x)\n",
    "        img = opt_x_rescaled.reshape((28,28))\n",
    "        k = k + 1\n",
    "        ax = fig.add_subplot(5, 5, k)\n",
    "        ax.imshow(img, cmap='gray', interpolation='none')\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "        w = auto_encoder.hidden_layer.W.get_value()[i,:]\n",
    "        w = w / np.sqrt((w ** 2).sum())\n",
    "        print (\"||x||^2 is {v:.2f}, activation is {a:.4f}, {t:.4f} (optimal)\"\n",
    "            .format(v=(opt_x**2).sum(), a=act_func(opt_x)[i], t=act_func(w)[i]))\n",
    "    plt.show()\n",
    "    fig.savefig('visual_ae2.png', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](visual_autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hinge loss\n",
    "\n",
    "||x||^2 is 1.00, activation is 0.0472, 0.2344 (optimal)\n",
    "\n",
    "||x||^2 is 1.00, activation is 0.1489, 0.5426 (optimal)\n",
    "\n",
    "||x||^2 is 1.00, activation is 0.0214, 0.1262 (optimal)\n",
    "\n",
    "||x||^2 is 1.00, activation is 0.4031, 0.7753 (optimal)\n",
    "\n",
    "||x||^2 is 1.00, activation is 0.0803, 0.4066 (optimal)\n",
    "\n",
    "||x||^2 is 1.00, activation is 0.0082, 0.0419 (optimal)\n",
    "\n",
    "||x||^2 is 1.00, activation is 0.0037, 0.0211 (optimal)\n",
    "\n",
    "||x||^2 is 1.00, activation is 0.4310, 0.8733 (optimal)\n",
    "\n",
    "||x||^2 is 1.00, activation is 0.9627, 0.9911 (optimal)\n",
    "\n",
    "![image](ae_hinge.png)\n",
    "\n",
    "#### Smoothed hinge loss\n",
    "\n",
    "||x||^2 is 1.00, activation is 0.2345, 0.2344 (optimal)\n",
    "\n",
    "||x||^2 is 1.00, activation is 0.5429, 0.5426 (optimal)\n",
    "\n",
    "||x||^2 is 1.00, activation is 0.1263, 0.1262 (optimal)\n",
    "\n",
    "||x||^2 is 1.00, activation is 0.7755, 0.7753 (optimal)\n",
    "\n",
    "||x||^2 is 1.00, activation is 0.4070, 0.4066 (optimal)\n",
    "\n",
    "||x||^2 is 1.00, activation is 0.0419, 0.0419 (optimal)\n",
    "\n",
    "||x||^2 is 1.00, activation is 0.0211, 0.0211 (optimal)\n",
    "\n",
    "||x||^2 is 1.00, activation is 0.8734, 0.8733 (optimal)\n",
    "\n",
    "||x||^2 is 1.00, activation is 0.9911, 0.9911 (optimal)\n",
    "\n",
    "![image](ae_smoothed_hinge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to \n",
    "* force the hidden layer to discover statistical dependencies between the inputs (aka. data/pattern structure) and\n",
    "* to prevent it from simply learning the identify function, \n",
    "\n",
    "we train the autoencoder to reconstruct the corrupted input.\n",
    "\n",
    "* To convert the autoencoder into a denoising autoencoder, all we need to do is to add a stochastic corruption step operating on the input.\n",
    "\n",
    "* To stochastic corruption process randomly sets some of the inputs (as many as half of them) to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Deep Networks for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Advantages of Deep Networks\n",
    "* Deep network can compactly represent a significantly larger set of functions than shallow networks.\n",
    "    * For a network with $n$ inputs, $l$ hidden layers and sigmoid activation, the bound of the complexity of the function implemented by a feedforward neural network is $2^{l-1}$. [M. Bianchini and F. Scarselli, On the complexity of shallow and deep neural network classifiers, ESANN14]\n",
    "    \n",
    "    ![image](apes-to-humans-evolution.jpg)\n",
    "    ![image](apes_aliens.png)\n",
    "    \n",
    "    \n",
    "* By building a deep network, one can start to learn part-whole decompositions in the case of images.\n",
    "    * The first layer might learn to detect edges,\n",
    "    * The second layer might learn to group edges to detect longer contours,\n",
    "    * The third layer might learn to detect simple parts of objects, and so on.\n",
    "    \n",
    "    ![image](part_whole.png)\n",
    "    \n",
    "    \n",
    "* Cortical computations in the brain also have multiple layers of processing.\n",
    "    * Visual images are processed in multiple stages by the brain, by cortical area \"V1\", followed by cortical area \"V2\", and so on.\n",
    "    \n",
    "    ![image](cortical.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difficulties of Training Deep Architectures\n",
    "\n",
    "The main learning algorithm that researchers were using was to randomly initialise the weights of the a deep network, and then train it using a labeled training set $\\{(x^{(i)}, y^{(i)}),i=1...m\\}$ using a supervised learning objective. However, this usually did not work well for deep networks.\n",
    "\n",
    "#### Availability of data\n",
    "* Labeled data is often scarce and sometimes expensive.\n",
    "* Given the high degree of expressive power of deep networks, training on insufficient data would result in overfitting.\n",
    "* For many problems, it is difficult to get enough samples to fit the parameters of a complex model.\n",
    "    * For example, face/biometric recognition, visual object recognition, speech recogition, many NLP problems, etc.\n",
    "    * An exception is the task of Click-Through Rate Prediction.\n",
    "\n",
    "#### Local optima\n",
    "* Training a deep network using supervised learning involves solving a highly non-convex optimisation problem, which is rife with bad local optima.\n",
    "* Training with gradient descent (or methods like conjugate gradient and L-BFGS) can easily get trapped in bad local optima.\n",
    "\n",
    "#### Diffusion of gradients\n",
    "* The gradients that are propagated backwards rapidly diminish in magnitude as the depth of the network increases.\n",
    "* When using gradient descent, the weights of the earlier layers change slowly, failing to learn much from the data.\n",
    "* Training a deep network ends up giving similar performance to training a shallow network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Layer-wise Training\n",
    "The main idea is \n",
    "* first to train the hidden layers of the network one at a time in an unsurpervised manner,\n",
    "* then to fine-tune the whole network with labeled data in a supervised manner.\n",
    "\n",
    "We use autoencoder as the hidden layers and softmax regression as the output layer in this section.\n",
    "![image](dnn.jpg)\n",
    "\n",
    "#### Fine-tuning\n",
    "\n",
    "Denote the activations of the last hidden layer for sample $x^{(i)}$ as $a^{(i)}$.\n",
    "Suppose we use softmax regression to classify the output of the last hidden layer.\n",
    "\n",
    "The loss function for fine-tuning on the training set $\\{(x^{(i)}, y^{(i)}), i=1...m\\}$ is\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\theta) = -\\frac{1}{m}\\left[\\sum_{i=1}^{m}\\sum_{j=1}^{k}1\\{y^{(i)}=j\\}\\log\\frac{e^{w_j^Ta^{(i)}+b_j}}{\\sum_{l=1}^{k}e^{w_l^Ta^{(i)}+b_j}}\\right] + \\frac{\\lambda}{2}\\sum_{i=1}^{k}\\sum_{j=1}^{n}w_{ij}^2\n",
    "\\end{equation}\n",
    "\n",
    "where $k$ is the number of units in the output layer, $n$ the number of units in the last hidden layer, and $w_{ij}$ and $b_j$ are the parameters of the softmax regression and $\\theta$ denotes all the paramters of the neural network including the ones of the softmax regression.\n",
    "\n",
    "#### Advantages of greedy layer-wise training\n",
    "\n",
    "##### Availabilty of data\n",
    "* Enable to take advantage of the cheap and plentiful unlabeled data\n",
    "\n",
    "##### Better local optima\n",
    "* The weights are now starting at a better location in parameter space than if they had been randomly initialised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model with Theano\n",
    "\n",
    "This section will implement a deep architecture with stacked autoencoders as the hidden layers and a softmax regression as the output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities for manipluating parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import numpy as np\n",
    "\n",
    "def get_size(shape):\n",
    "    \"\"\"\n",
    "    count the number of elements in a ndarray with shape=shape\n",
    "    \"\"\"\n",
    "    size = 1\n",
    "    for i in shape:\n",
    "        size *= i\n",
    "    return size\n",
    "    \n",
    "def pack(param_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        param_list: list of ndarrays\n",
    "    Returns:\n",
    "        theta:  vector of shape (?,), flattened params\n",
    "        shapes: list of tuples, with each tuple storing the shape of each ndarray in param_list\n",
    "    \"\"\"\n",
    "    shapes = []\n",
    "    theta=None\n",
    "    for p in param_list:\n",
    "        size = p.size\n",
    "        p2 = p.reshape((size, )) \n",
    "        if theta is None:\n",
    "            theta = p2\n",
    "        else:\n",
    "            theta = np.hstack((theta, p2))\n",
    "        shapes += [p.shape] \n",
    "    return theta, shapes\n",
    "        \n",
    "def unpack(theta, shapes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        theta:  vector of shape (?,), flattened params\n",
    "        shapes: list of tuples, with each tuple storing the shape of each ndarray in param_list      \n",
    "    Returns:\n",
    "        param_list: list of ndarrays\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    params = []\n",
    "    for shape in shapes:\n",
    "        size = get_size(shape)\n",
    "        x = theta[i:i+size]\n",
    "        params += [x.reshape(shape)]\n",
    "        i += size\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, n_in, n_out, name=''):\n",
    "        r = -np.sqrt(6.0 / (n_in + n_out + 1))\n",
    "        rand_W = 2 * r * np.random.rand(n_out, n_in) - r\n",
    "        self.W = theano.shared(\n",
    "            value=rand_W,\n",
    "            name='W_'+name,\n",
    "            borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            value=np.zeros((n_out,), dtype=theano.config.floatX),\n",
    "            name='b_'+name,\n",
    "            borrow=True)\n",
    "        self.params = [self.W, self.b]\n",
    "        self.L2_sqr = (self.W ** 2).sum()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: matrix tensor, each column stores a sample\n",
    "        \"\"\"\n",
    "        linear_input = (T.dot(self.W, X).transpose() + self.b).transpose()\n",
    "        activation = T.nnet.sigmoid(linear_input)\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AutoEncoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import gzip\n",
    "import cPickle\n",
    "\n",
    "#from param_util import pack, unpack\n",
    "#from layer import Layer\n",
    "\n",
    "# for debugging\n",
    "theano.config.optimizer='fast_run'#'fast_compile'\n",
    "theano.config.exception_verbosity=\"high\"\n",
    "\n",
    "class AutoEncoder(object):\n",
    "    def __init__(self, n_in, n_hid, \n",
    "                 L2_reg_coef, sparse_reg_coef, sparse_rho, \n",
    "                 max_iter=400):\n",
    "        self.n_in = n_in\n",
    "        self.n_hid = n_hid\n",
    "        self.L2_reg_coef = L2_reg_coef\n",
    "        self.sparse_reg_coef = sparse_reg_coef\n",
    "        self.sparse_rho = sparse_rho\n",
    "        self.max_iter = max_iter \n",
    "        \n",
    "        self.hidden_layer = Layer(self.n_in, self.n_hid, 'hidden')\n",
    "        self.output_layer = Layer(self.n_hid, self.n_in, 'output')\n",
    "        self.params = self.hidden_layer.params + self.output_layer.params\n",
    "        \n",
    "        X = T.dmatrix(name='X') # each column stores a sample\n",
    "        m = X.shape[1] # sample count\n",
    "        activation = self.hidden_layer.forward(X)\n",
    "        output = self.output_layer.forward(activation)\n",
    "        error = ((output - X) ** 2).sum() / (2.0 * m)\n",
    "        L2_reg = self.hidden_layer.L2_sqr + self.output_layer.L2_sqr\n",
    "        rho = activation.mean(axis=1)\n",
    "        kl = (self.sparse_rho * T.log(self.sparse_rho/rho) + \n",
    "              (1-self.sparse_rho) * T.log((1-self.sparse_rho)/(1-rho))).sum()\n",
    "        cost = error + self.L2_reg_coef * 0.5 * L2_reg + self.sparse_reg_coef * kl\n",
    "        grads = T.grad(cost, self.params)\n",
    "        params = [p.type() for p in self.params]\n",
    "        self.cost_func = theano.function(\n",
    "            inputs=params + [X],\n",
    "            outputs=cost,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "        self.grad_func = theano.function(\n",
    "            inputs=params + [X],\n",
    "            outputs=grads,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "        self.run_model = theano.function(\n",
    "            inputs=[X],\n",
    "            outputs=activation)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: matrix tensor, each column stores a sample\n",
    "        \"\"\"\n",
    "        return self.hidden_layer.forward(X)\n",
    "            \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        X: np.ndarray, each column stores a sample\n",
    "        \"\"\"\n",
    "        init_theta, shapes = pack([\n",
    "            self.hidden_layer.W.get_value(), self.hidden_layer.b.get_value(), \n",
    "            self.output_layer.W.get_value(), self.output_layer.b.get_value()])\n",
    "        opt_theta, opt_cost, d = sp.optimize.fmin_l_bfgs_b(\n",
    "            func=self.cost_and_grad,\n",
    "            x0=init_theta,\n",
    "            fprime=None,\n",
    "            args=(shapes, X),\n",
    "            maxiter=self.max_iter,\n",
    "            iprint=1)\n",
    "        opt_param_list = unpack(opt_theta, shapes)\n",
    "        for shared_param, opt_param in zip(self.params, opt_param_list):\n",
    "             shared_param.set_value(opt_param)\n",
    "             \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        X: np.ndarray, each column stores a sample\n",
    "        \"\"\"\n",
    "        return self.run_model(X)\n",
    "\n",
    "    def cost_and_grad(self, theta, *args):\n",
    "        \"\"\"\n",
    "        compute the cost and gradient at theta in the parameter space\n",
    "        args should be (shapes, X)\n",
    "        X: each column stores a sample\n",
    "        \"\"\"\n",
    "        shapes = args[0]\n",
    "        X = args[1]\n",
    "        param_list = unpack(theta, shapes)\n",
    "        cost =  self.cost_func(*(param_list+[X]))\n",
    "        grad = self.grad_func(*(param_list+[X]))\n",
    "        grad, _ = pack(grad)\n",
    "        return cost, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SoftmaxRegression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import gzip\n",
    "import cPickle\n",
    "\n",
    "from param_util import pack, unpack\n",
    "\n",
    "# for debugging\n",
    "theano.config.optimizer=\"fast_run\"\n",
    "theano.config.exception_verbosity=\"high\"\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "    def __init__(self, n_in, n_out, L2_reg_coef, max_iter=100):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.L2_reg_coef = L2_reg_coef\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        self.W = theano.shared(\n",
    "            value=0.005 * np.random.randn(self.n_out, self.n_in),\n",
    "            name='W',\n",
    "            borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            value=np.zeros((self.n_out, ), dtype=theano.config.floatX),\n",
    "            name='b',\n",
    "            borrow=True)\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "        # sample matrix, each column stores a sample\n",
    "        X = T.dmatrix('X')\n",
    "        # label\n",
    "        y = T.lvector('y')\n",
    "        # predict\n",
    "        pred, p_y_given_x = self.predict(X)\n",
    "        # cost\n",
    "        cost = self.calc_cost(p_y_given_x, y)\n",
    "        # error rate\n",
    "        error = 1.0*T.sum(T.neq(pred, y))/X.shape[1]\n",
    "        \n",
    "        self.test_model = theano.function(\n",
    "            inputs=[X, y],\n",
    "            outputs=error)\n",
    "        self.run_model = theano.function(\n",
    "            inputs=[X],\n",
    "            outputs=pred)\n",
    "            \n",
    "        # compute gradient\n",
    "        grads = T.grad(cost, self.params)\n",
    "        params = [p.type() for p in self.params]\n",
    "        self.grad_func = theano.function(\n",
    "            inputs=params + [X, y],\n",
    "            outputs=grads,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "        self.cost_func = theano.function(\n",
    "            inputs=params + [X, y],\n",
    "            outputs=cost,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "    \n",
    "    def calc_cost(self, p_y_given_x, y):\n",
    "        \"\"\"\n",
    "        p_y_given_x: matrix tensor, output by self.predict\n",
    "        y: vector tensor, labels\n",
    "        \"\"\"\n",
    "        # NLL: negative log-likelihood\n",
    "        nll = -T.mean(T.log(p_y_given_x[y, T.arange(0, y.shape[0])]))\n",
    "        cost = nll + self.L2_reg_coef * (self.W**2).sum()\n",
    "        return cost\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: matrix tensor, each column stores a sample\n",
    "        \"\"\"\n",
    "        linear_input = (T.dot(self.W, X).transpose() + self.b).transpose()\n",
    "        p_y_given_x = self.softmax(linear_input)\n",
    "        pred = T.argmax(p_y_given_x, axis=0)\n",
    "        return pred, p_y_given_x        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        init_theta, shapes = pack([self.W.get_value(), self.b.get_value()])\n",
    "        opt_theta, opt_cost, d = sp.optimize.fmin_l_bfgs_b(\n",
    "            func=self.cost_and_grad,\n",
    "            x0=init_theta,\n",
    "            fprime=None,\n",
    "            args=(shapes, X, y),\n",
    "            maxiter=self.max_iter,\n",
    "            iprint=1)\n",
    "        opt_param_list = unpack(opt_theta, shapes)\n",
    "        for shared_param, opt_param in zip(self.params, opt_param_list):\n",
    "             shared_param.set_value(opt_param)      \n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.run_model(X)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        return self.test_model(X, y)\n",
    "        \n",
    "    def cost_and_grad(self, theta, *args):\n",
    "        shapes = args[0]\n",
    "        X = args[1]\n",
    "        y = args[2]\n",
    "        param_list = unpack(theta, shapes)\n",
    "        cost =  self.cost_func(*(param_list + [X, y]))\n",
    "        grad = self.grad_func(*(param_list + [X, y]))\n",
    "        grad, _ = pack(grad)\n",
    "        return cost, grad        \n",
    "    \n",
    "    def softmax(self, M):\n",
    "        \"\"\"\n",
    "        normalise along the vertical axis\n",
    "        \"\"\"\n",
    "        e_M = T.exp(M - M.max(axis=0, keepdims=True))\n",
    "        return e_M / e_M.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import gzip\n",
    "import cPickle\n",
    "\n",
    "#from param_util import pack, unpack\n",
    "#from autoencoder import AutoEncoder\n",
    "#from softmax_regression import SoftmaxRegression\n",
    "\n",
    "# for debugging\n",
    "theano.config.optimizer='fast_run'#'fast_compile'\n",
    "theano.config.exception_verbosity=\"high\"\n",
    "\n",
    "class DNN(object):\n",
    "    \"\"\"\n",
    "    stacked autoencoder with a softmax output layer\n",
    "    \"\"\"\n",
    "    def __init__(self, ae_layers, softmax_layer, finetune_max_iter=100, prefix='dnn'):\n",
    "        self.ae_layers = ae_layers\n",
    "        self.softmax_layer = softmax_layer\n",
    "        self.finetune_max_iter = finetune_max_iter\n",
    "        \n",
    "        X = T.dmatrix(name=prefix+'_X')\n",
    "        y = T.lvector(name=prefix+'y')\n",
    "        \n",
    "        # forward propagation through AE layers\n",
    "        self.params = []        \n",
    "        activations = [X]    \n",
    "        for layer in self.ae_layers:\n",
    "            activations += [layer.forward(activations[-1])]\n",
    "            self.params += layer.hidden_layer.params\n",
    "        \n",
    "        # forward progagation through output layer\n",
    "        pred, p_y_given_x = self.softmax_layer.predict(activations[-1])\n",
    "        self.params += softmax_layer.params\n",
    "        \n",
    "        # cost\n",
    "        cost = self.softmax_layer.calc_cost(p_y_given_x, y)\n",
    "        # error\n",
    "        error = 1.0*T.sum(T.neq(pred, y))/X.shape[1]\n",
    "        \n",
    "        grads = T.grad(cost, self.params)\n",
    "        params = [p.type() for p in self.params]\n",
    "        \n",
    "        self.cost_func = theano.function(\n",
    "            inputs=params + [X, y],\n",
    "            outputs=cost,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "        self.grad_func = theano.function(\n",
    "            inputs=params + [X, y],\n",
    "            outputs=grads,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "        self.test_model = theano.function(\n",
    "            inputs=[X, y],\n",
    "            outputs=error)\n",
    "        self.run_model = theano.function(\n",
    "            inputs=[X],\n",
    "            outputs=pred)\n",
    "            \n",
    "    def pretrain(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.ndarray, each column stores a sample\n",
    "        \"\"\"\n",
    "        X_in = X\n",
    "        for layer in self.ae_layers:\n",
    "            layer.fit(X_in)\n",
    "            X_out = layer.transform(X_in)\n",
    "            X_in = X_out\n",
    "            \n",
    "        self.softmax_layer.fit(X_in, y)\n",
    "    \n",
    "    def finetune(self, X, y):\n",
    "        init_theta, shapes = pack(map(lambda p: p.get_value(), self.params))\n",
    "        opt_theta, opt_cost, d = sp.optimize.fmin_l_bfgs_b(\n",
    "            func=self.cost_and_grad,\n",
    "            x0=init_theta,\n",
    "            fprime=None,\n",
    "            args=(shapes, X, y),\n",
    "            maxiter=self.finetune_max_iter,\n",
    "            iprint=1)\n",
    "        opt_param_list = unpack(opt_theta, shapes)\n",
    "        for shared_param, opt_param in zip(self.params, opt_param_list):\n",
    "             shared_param.set_value(opt_param)\n",
    "                 \n",
    "    def fit(self, X, y):\n",
    "        self.pretrain(X, y)\n",
    "        self.finetune(X, y)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.run_model(X)\n",
    "  \n",
    "    def evaluate(self, X, y):\n",
    "        return self.test_model(X, y)\n",
    "\n",
    "    def cost_and_grad(self, theta, *args):\n",
    "        \"\"\"\n",
    "        compute the cost and gradient at theta in the parameter space\n",
    "        args should be (shapes, X, y)\n",
    "        X: each column stores a sample\n",
    "        \"\"\"\n",
    "        shapes = args[0]\n",
    "        X = args[1]\n",
    "        y = args[2]\n",
    "        param_list = unpack(theta, shapes)\n",
    "        cost =  self.cost_func(*(param_list+[X, y]))\n",
    "        grad = self.grad_func(*(param_list+[X, y]))\n",
    "        grad, _ = pack(grad)\n",
    "        return cost, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the deep network for digit classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    f = gzip.open('data/mnist.pkl.gz')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    \n",
    "    # train model\n",
    "    train_X = train_set[0].transpose()\n",
    "    train_y = train_set[1]\n",
    "    valid_X = valid_set[0].transpose()\n",
    "    valid_y = valid_set[1]\n",
    "    \n",
    "    train_X = np.hstack((train_X, valid_X))\n",
    "    train_y = np.hstack((train_y, valid_y))\n",
    "    \n",
    "    train_X = train_X[:, 0:60001:5]\n",
    "    train_y = train_y[0:60001:5]\n",
    "    \n",
    "    n_in = train_X.shape[0]\n",
    "    n_out = np.unique(train_y).shape[0]\n",
    "    print \"{n} training samples of dim {d}\".format(n=train_X.shape[1], d=n_in)\n",
    "    \n",
    "    auto_encoder1 = AutoEncoder(\n",
    "        n_in=train_X.shape[0],\n",
    "        n_hid=200,\n",
    "        L2_reg_coef=3e-3,\n",
    "        sparse_reg_coef=3.0,\n",
    "        sparse_rho=0.1,\n",
    "        max_iter=200)  \n",
    "    auto_encoder2 = AutoEncoder(\n",
    "        n_in=auto_encoder1.n_hid,\n",
    "        n_hid = 200,\n",
    "        L2_reg_coef=3e-3,\n",
    "        sparse_reg_coef=3.0,\n",
    "        sparse_rho=0.1,\n",
    "        max_iter=200) \n",
    "    softmax_regression = SoftmaxRegression(\n",
    "        n_in=auto_encoder2.n_hid, \n",
    "        n_out=10, \n",
    "        L2_reg_coef=3e-3, \n",
    "        max_iter=100)\n",
    "    dnn = DNN((auto_encoder1, auto_encoder2), softmax_regression, finetune_max_iter=100)\n",
    "    dnn.fit(train_X, train_y)\n",
    "    \n",
    "    # test model\n",
    "    test_X = test_set[0].transpose()\n",
    "    test_y = test_set[1]\n",
    "    #pred = dnn.transform(test_X)\n",
    "    #error = 1.0*(pred != test_y).sum()/test_y.shape[0]\n",
    "    error = dnn.evaluate(test_X, test_y)\n",
    "    print 'error rate on test set is {e}%, accuracy is {a}%'.format(e=error*100, a=100-100*error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimental results\n",
    "\n",
    "Training set:\n",
    "* 60,000 handwritten digit images of 0~9,\n",
    "* Selecting a subset of the training samples by picking one for every five samples.\n",
    "\n",
    "Test set:\n",
    "* 10,000 handwritten digit images of 0~9.\n",
    "\n",
    "<table>\n",
    "<tbody>\n",
    "<tr><td>** Method **</td><td>** Accuracy **</td></tr>\n",
    "<tr><td>Baseline</td><td>9.8%</td></tr>\n",
    "<tr><td>Softmax Regression</td><td>91.5%</td></tr>\n",
    "<tr><td>DNN (1 AE)(pretraining)</td><td>92.4%</td></tr>\n",
    "<tr><td>DNN (1 AE)(fine-tuning)</td><td>93.83%</td></tr>\n",
    "<tr><td>DNN (1 AE)(pretaining plus fine-tuning)</td><td>95.0%</td></tr>\n",
    "<tr><td>DNN (2 AEs)(pretraining)</td><td>85.9%</td></tr>\n",
    "<tr><td>DNN (2 AEs)(fine-tuning)</td><td>93.77%</td></tr>\n",
    "<tr><td>** DNN (2 AEs)(pretaining plus fine-tuning) **</td><td>** 95.9% **</td></tr>\n",
    "</table>\n",
    "</tbody>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights into the Behaviours of the Neurons\n",
    "\n",
    "Find input $x$ that maximally activates a specific hidden neuron and has limited $L_2$ norm: \n",
    "$$\n",
    "\\max_{x}a^{(l)}_i(x)\\\\\n",
    "s.t. \\Vert x \\Vert^2 \\le 1\n",
    "$$\n",
    "This problem can be approximated by a non-constrained minimisation problem:\n",
    "$$\n",
    "\\min_{x}\\left[-a^{(l)}_i(x) + \\lambda \\max(\\Vert x \\Vert^2 - 1, 0)\\right]\n",
    "$$\n",
    "where $\\lambda$ is a fairly large number.\n",
    "\n",
    "$\\mathcal{h}(x)=\\max(\\Vert x \\Vert^2 - 1, 0)$ can be considered as a hinge loss. Since the derivative of the hinge loss at $\\Vert x \\Vert^2 = 1$ is non-deterministic, smoothed versions may be preferred for more stable optimisation.\n",
    "\n",
    "#### Hinge loss function\n",
    "\n",
    "$\\mathcal{h}(x)=\\max(\\Vert x \\Vert^2 - 1, 0)$\n",
    "\n",
    "As the optimal solution may very well lie in the neighborhood of $\\Vert x \\Vert^2 = 1$, the loss gradient therein should be more \"cautious\" as it approaches the border to avoid bouncing back and forth in vain.\n",
    "\n",
    "#### Smoothed hinge loss function\n",
    "\n",
    "$\\mathcal{h}_s(x)=\\frac{1}{2\\gamma}\\max(\\Vert x \\Vert^2 - 1, 0)^2$\n",
    "\n",
    "* For $x$ that $\\Vert x \\Vert_2 > \\sqrt{2}$, push $x$ harder than the hinge loss.\n",
    "\n",
    "* For $x$ that $\\Vert x \\Vert_2 < \\sqrt{2}$, push $x$ more gentle than the hinge loss.\n",
    "\n",
    "![image](hinge_loss_plot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visulisation the first hidden layer\n",
    "plt.close('all')\n",
    "def rescale(x):\n",
    "    # Remove DC (mean of images)\n",
    "    x = x - x.mean()\n",
    "    # Truncate to +/-3 standard deviations and scale to -1 to 1\n",
    "    pstd = 3 * x.std()\n",
    "    x = np.maximum(np.minimum(x, pstd), -pstd) / pstd\n",
    "    # Rescale from [-1,1] to [0.1,0.9]\n",
    "    x = (x + 1) * 0.4 + 0.1;\n",
    "    return x\n",
    "fig = plt.figure(0)\n",
    "k = 0\n",
    "for i in range(0, 64):#for i in np.random.randint(0, dnn.ae_layers[0].n_hid, 64):\n",
    "    x = T.dvector('x')\n",
    "    x0 = np.random.rand(28*28)\n",
    "    x0 = x0 / np.sqrt((x0**2).sum())\n",
    "    a = dnn.ae_layers[0].forward(x)\n",
    "    # cost\n",
    "    cost = -a[i] + 1e2 * T.max([0, (x**2).sum()-1])**2\n",
    "    grad = T.grad(cost, x)\n",
    "    cost_func = theano.function([x], cost)\n",
    "    grad = T.grad(cost, x)\n",
    "    grad_func = theano.function([x], grad)\n",
    "    act_func = theano.function([x], a)\n",
    "    def cost_and_grad(x):\n",
    "        return cost_func(x), grad_func(x)\n",
    "    opt_x, opt_cost, d = sp.optimize.fmin_l_bfgs_b(\n",
    "        func=cost_and_grad,\n",
    "        x0=x0,\n",
    "        maxiter=1000,\n",
    "        iprint=0)\n",
    "    print (\"||x||^2 is {v:.2f}, activation is {a:.2f}\"\n",
    "            .format(v=(opt_x**2).sum(), a=act_func(opt_x)[i]))\n",
    "    opt_x_rescaled = rescale(opt_x)\n",
    "    img = opt_x_rescaled.reshape((28,28))\n",
    "    k = k + 1\n",
    "    ax = fig.add_subplot(8, 8, k)\n",
    "    ax.imshow(img, cmap='gray', interpolation='none')\n",
    "    ax.set_axis_off()\n",
    "plt.show()\n",
    "fig.savefig('visual_dnn_layer1.png', format='png')\n",
    "\n",
    "# visulisation the second hidden layer\n",
    "fig = plt.figure(1)\n",
    "k = 0\n",
    "for i in range(0, 64):#np.random.randint(0, dnn.ae_layers[1].n_hid, 64):\n",
    "    x = T.dvector('x')\n",
    "    x0 = np.random.rand(28*28)\n",
    "    x0 = x0 / np.sqrt((x0**2).sum())\n",
    "    a1 = dnn.ae_layers[0].forward(x)\n",
    "    a = dnn.ae_layers[1].forward(a1)\n",
    "    # cost\n",
    "    cost = -a[i] + 1e2 * T.max([0, (x**2).sum()-1])**2\n",
    "    grad = T.grad(cost, x)\n",
    "    cost_func = theano.function([x], cost)\n",
    "    grad = T.grad(cost, x)\n",
    "    grad_func = theano.function([x], grad)\n",
    "    act_func = theano.function([x], a)\n",
    "    def cost_and_grad(x):\n",
    "        return cost_func(x), grad_func(x)\n",
    "    opt_x, opt_cost, d = sp.optimize.fmin_l_bfgs_b(\n",
    "        func=cost_and_grad,\n",
    "        x0=x0,\n",
    "        maxiter=1000,\n",
    "        iprint=0)\n",
    "    #print opt_x\n",
    "    print (\"||x||^2 is {v:.2f}, activation is {a:.2f}\"\n",
    "            .format(v=(opt_x**2).sum(), a=act_func(opt_x)[i]))        \n",
    "    opt_x_rescaled = rescale(opt_x)\n",
    "    img = opt_x_rescaled.reshape((28,28))\n",
    "    k = k + 1\n",
    "    ax = fig.add_subplot(8, 8, k)\n",
    "    ax.imshow(img, cmap='gray', interpolation='none')\n",
    "    ax.set_axis_off()\n",
    "plt.show()\n",
    "fig.savefig('visual_dnn_layer2.png', format='png')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs that maximise the activations of the first 64 neursons ** in the first hidden layer after pretraining **:\n",
    "![image](visual_dnn_layer1_pretrain.png)\n",
    "\n",
    "Inputs that maximise the activations of of the first 64 neursons ** in the second hidden layer after pretraining **:\n",
    "![image](visual_dnn_layer2_pretrain.png)\n",
    "\n",
    "Inputs that maximise the activations of the first 64 neursons ** in the first hidden layer after fine-tuning **:\n",
    "![image](visual_dnn_layer1_finetune.png)\n",
    "\n",
    "Inputs that maximise the activations of of the first 64 neursons ** in the second hidden layer after fine-tuning **:\n",
    "![image](visual_dnn_layer2_finetune.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generative Model v.s. Discriminative Model\n",
    "* Generative Model\n",
    "    * Tries to model the distribution of data in the feature space\n",
    "    * More explainable\n",
    "* Discriminative Model\n",
    "    * Tries to model the differences among patterns\n",
    "    * Less explainable\n",
    "\n",
    "An example is to recognise two handwritten digits $0$ and $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivations\n",
    "\n",
    "CNNs are biologically-inspired variants of MLPs.\n",
    "\n",
    "* The visual cortex cells are sensitive to small sub-regions of the visual field, called a ** receptive field **.\n",
    "\n",
    "* These cells act as local filters over the input space.\n",
    "\n",
    "* Simple cells respond maximally to specific edge-like patterns within their receptive field.\n",
    "\n",
    "* Complex cells have large receptive fields and are locally invariant to the exact position of the pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Connectivity\n",
    "\n",
    "The inputs of hidden units in layer $L_m$ are from a subset of units in layer $L_{m-1}$.\n",
    "\n",
    "Stacking many such layers leads to \"filters\" that become increasingly \"global\".\n",
    "\n",
    "![image](sparse_1D_nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Weights and Convolution\n",
    "\n",
    "Each filter $h_i$ is replicated across the entire visual field. \n",
    "\n",
    "These replicated units share the same parameterization (weight vector and bias) and form a **feature map**.\n",
    "\n",
    "![image](conv_1D_nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we denote the $k$-th feature map at a given layer as $h^k$, whose filters are determined by the weights $W^k$ and bias $b_k$, then the feature map $h^k$ is obtained as follows:\n",
    "\n",
    "$$h^k_{ij} = sigmoid ( (W^k * x)_{ij} + b_k )$$\n",
    "\n",
    "![image](convolution_schematic.gif)\n",
    "\n",
    "To form a richer representation of the data, each hidden layer is composed of multiple feature maps, $\\{h^{(k)}, k=0..K\\}$. \n",
    "\n",
    "The weights $W$ of a hidden layer can be represented in a **4D tensor**.\n",
    "\n",
    "![image](cnn_explained.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxPooling\n",
    "\n",
    "Max-pooling is a form of non-linear down-sampling.\n",
    "\n",
    "Max-pooling partitions the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum value.\n",
    "\n",
    "![image](Pooling_schematic.gif)\n",
    "\n",
    "Max-pooling is useful in vision for two reasons:\n",
    "\n",
    "* It reduces computation for upper layers by eliminating non-maximal values.\n",
    "\n",
    "* It provides a form of translation invariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Full Model: LeNet\n",
    "\n",
    "Sparse, convolutional layers and max-pooling are at the heart of the LeNet family of models.\n",
    "\n",
    "* The lower-layers are composed to alternating convolution and max-pooling layers. \n",
    "\n",
    "* The upper-layers however are fully-connected and correspond to a traditional MLP (hidden layer + logistic/softmax regression).\n",
    "\n",
    "![image](mylenet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Belief Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restricted Boltzmann Machines (RBMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Energy-Based Models (EMB)\n",
    "\n",
    "Energy-based probabilistic models define a probability distribution through an energy function $E(\\cdot)$,\n",
    "\n",
    "$$p(x) = \\frac{e^{-E(x)}}{Z}$$\n",
    "\n",
    "where the normalising factor $Z$ is called the partition function,\n",
    "\n",
    "$$Z = \\sum_{x}{e^{-E(x)}}$$\n",
    "\n",
    "Given a dataset $\\mathcal{D} = \\{x^{(i)}\\}$, the negative log-likelihood function is\n",
    "\n",
    "$$\\mathcal{l}(\\theta) = -\\sum_{x\\in \\mathcal{D}}\\log{p(x)}$$\n",
    "\n",
    "#### EMB with hidden units\n",
    "\n",
    "$$p(x) = \\sum_{h}{p(x,h)} = \\sum_{h}{\\frac{e^{-E(x,h)}}{Z}}$$\n",
    "\n",
    "By defining the free energy function $F(x)$ as\n",
    "\n",
    "$$F(x) = -\\log{\\sum_{h}{e^{-E(x,h)}}}$$\n",
    "\n",
    "$p(x)$ can be rewritten as\n",
    "\n",
    "$$p(x) = \\frac{e^{-F(x)}}{Z}$$\n",
    "\n",
    "To minimise the negative log-likelihood function $\\mathcal{l}(\\theta)$ by gradient-based optimisation alogrithms,\n",
    "\n",
    "$$\\frac{\\partial{\\mathcal{l}(\\theta)}}{\\partial{\\theta}} = \\sum_x{\\left(-\\frac{\\partial{\\log{p(x)}}}{\\partial{\\theta}}\\right)}$$\n",
    "\n",
    "$$-\\frac{\\partial{\\log{p(x)}}}{\\partial{\\theta}} = \\frac{\\partial F(x)}{\\partial \\theta} - \\sum_{\\hat{x}}p(\\hat{x})\\frac{\\partial F(\\hat{x})}{\\partial \\theta}$$\n",
    "\n",
    "The first step to make this computation tractable is to estimate the expectation $E_{p(x)}\\frac{\\partial F(\\hat{x})}{\\partial \\hat{x}} = \\sum_{\\hat{x}}p(\\hat{x})\\frac{\\partial F(\\hat{x})}{\\partial \\hat{x}}$ using a number of particles sampled from $p(x)$.\n",
    "\n",
    "$$-\\frac{\\partial{\\log{p(x)}}}{\\partial{\\theta}} \\approx \\frac{\\partial F(x)}{\\partial \\theta} - \\frac{1}{|\\mathcal{N}|}\\sum_{\\bar{x}\\in \\mathcal{N}}\\frac{\\partial F(\\bar{x})}{\\partial \\theta}$$\n",
    "\n",
    "Challenges are\n",
    "\n",
    "* The calculation of $F(x)$ and $\\frac{\\partial F(x)}{\\partial\\theta}$\n",
    "\n",
    "* Sampling particles from the distribution $p(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restricted Boltzmann Machines (RBM)\n",
    "\n",
    "Restricted Boltzmann Machines restrict BMs to those without visiblevisible and hiddenhidden connections.\n",
    "\n",
    "![image](RBM.png)\n",
    "\n",
    "Properties:\n",
    "\n",
    "$$p(h|v) = \\prod_i p(h_i|v)$$\n",
    "$$p(v|h) = \\prod_j p(v_j|h)$$\n",
    "\n",
    "The energy function $E(v, h)$ is defined as\n",
    "\n",
    "$$E(v,h) = -b^Tv - c^Th - h^TWv$$\n",
    "\n",
    "$$F(v) = -b^Tv - \\sum_{i}\\log\\sum_{h_i}e^{h_i(c_i + W_iv)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RBMs with binary units\n",
    "\n",
    "In the commonly studied case of using binary units (where $v_j$ and $h_i \\in\n",
    "\\{0,1\\}$), we have\n",
    "\n",
    "$$P(h_i=1|v) = sigmoid(c_i + W_i v)$$\n",
    "\n",
    "$$P(v_j=1|h) = sigmoid(b_j + W^T_j h)$$\n",
    "\n",
    "$$F(v)= - b^Tv - \\sum_i \\log(1 + e^{(c_i + W_i v)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling in an RBM\n",
    "\n",
    "Samples of $p(x)$ can be obtained by running a Markov chain to convergence, using Gibbs sampling as the transition operator.\n",
    "\n",
    "A step in the Markov chain is taken as follows:\n",
    "\n",
    "$$h^{(n+1)} \\sim sigmoid(W^Tv^{(n)} + c)$$\n",
    "\n",
    "$$v^{(n+1)} \\sim sigmoid(W h^{(n+1)} + b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Belief Networks\n",
    "\n",
    "Unsupervised generative pre-training of stacked RBMs followed by supervised fine-tuning.\n",
    "\n",
    "![image](DBN3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
