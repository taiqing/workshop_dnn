{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a Taste of DNN on the Shoulder of Theano\n",
    "===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](fruit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "* Introduction to Theano\n",
    "* Softmax regression\n",
    "* Highlights of DNN\n",
    "* Neural network\n",
    "    * Multiple layer perception\n",
    "    * Forward propagation\n",
    "    * Backward propagation\n",
    "* Sparse autoencoder\n",
    "    * Autoencoder\n",
    "    * Sparse autoencoder\n",
    "* Building deep networks for classification\n",
    "    * Model structure\n",
    "    * Pre-training\n",
    "    * Fine-tuning\n",
    "    * Experimental results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Theano\n",
    "Theano is Python library that allows you to define, evaluate and optimize math expressions.\n",
    "* Efficient symbolic differentiation\n",
    "* Efficient handling of matrices\n",
    "* Tight integration of NumPy\n",
    "* Dynamic C code generate\n",
    "* Transparent use of GPU\n",
    "\n",
    "#### Fast to develop and fast to run\n",
    "![image](fast.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine learning libraries built on top of Theano:\n",
    "* Pylearn2\n",
    "    * great flexibility and a good choice for trying out ML ideas\n",
    "* PyMC3\n",
    "    * Probabilistic programming; building statistical Bayesian models\n",
    "* Sklearn-theano\n",
    "    * Easy-to-use deep learning tool\n",
    "* Lasagne\n",
    "    * Lightweight library to build neural networks\n",
    "\n",
    "#### Models that have been built with Theano:\n",
    "* Neural networks\n",
    "* Convolutional Neural Networks (CNN)\n",
    "* Recurrent Neural Networks (RNN)\n",
    "* Long Short Term Memory (LSTM)\n",
    "* Autoencoders\n",
    "* GoogLeNet\n",
    "* Overfeat\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symbolic variables in Theano\n",
    "* Variable (C, Java, Python, etc.)\n",
    "    * A segment of physical storage in RAM\n",
    "    * Operations are based on value passing between variables\n",
    "* Tensor (Theano)\n",
    "    * A mathematical symbol\n",
    "    * No physical storage in RAM to hold its value\n",
    "    * Operations are actually building connections between tensors\n",
    "* Shared variable (Theano)\n",
    "    * Hybrid of variable and tensor\n",
    "    * Tensor with physical storage in RAM to hold its value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output file is available at 1.png\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "x = T.dvector(name='x')\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "y = f(x)\n",
    "\n",
    "theano.printing.pydotprint(theano.function([x], y), '1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_theano.function_ brings life to theano variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is [ 1.  2.  3.], b is [ 1.  4.  9.]\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "\n",
    "x = T.dvector(name='x')\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "y = f(x)\n",
    "\n",
    "pow2 = theano.function(inputs=[x], outputs=y)\n",
    "\n",
    "a = np.array([1,2,3], dtype=theano.config.floatX)\n",
    "b = pow2(a)\n",
    "print \"a is {a}, b is {b}\".format(a=a, b=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 1.,  1.,  1.])]\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "\n",
    "x = T.dvector('x')\n",
    "y = x.sum()\n",
    "grad = T.grad(cost=y, wrt=[x])\n",
    "grad_func = theano.function(inputs=[x], outputs=grad)\n",
    "\n",
    "a = np.array([1,2,3], dtype=theano.config.floatX)\n",
    "print grad_func(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "In the softmax regression setting, we are interested in multi-class classification. Suppose we have $m$ samples in the training set $\\{(x^{(1)}, y^{(1)}),...,(x^{(m)}, y^{(m)})\\}$, where $y^{(i)}\\in \\{1,2,...,k\\}$ and $x^{(i)}\\in R^{n}$.\n",
    "\n",
    "Given a test sample $x$, we want to estimate the probability that $x$ belongs to class $j$, i.e., $p(y=j|x)$, for all possible $j$.\n",
    "\n",
    "\\begin{equation}\n",
    "h_{W,b}(x) = \n",
    "\\left[\n",
    "  \\begin{array}{c}\n",
    "  p(y=1|x;W,b)\\\\\n",
    "  p(y=2|x;W,b)\\\\\n",
    "  ...\\\\\n",
    "  p(y=k|x;W,b)\\\\\n",
    "  \\end{array}\n",
    "\\right]\n",
    "=\\frac{1}{\\sum_{j=1}^{k}{e^{w_j^Tx+b_j}}}\n",
    "\\left[\n",
    "  \\begin{array}{c}\n",
    "  e^{w_1^Tx+b_1}\\\\\n",
    "  e^{w_2^Tx+b_2}\\\\\n",
    "  ...\\\\\n",
    "  e^{w_k^Tx+b_k}\\\\\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{equation}\n",
    "\n",
    "When you implement softmax regression, it is usually convenient to represent $W$ as a $n$-by-$k$ matrix, so that\n",
    "\\begin{equation}\n",
    "W = \\left[\\begin{array}{c}\n",
    "w_1^T\\\\\n",
    "...\\\\\n",
    "w_k^T\n",
    "\\end{array}\\right]\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Defining a Loss Function\n",
    "The loss of $h_{W, b}$ on the trainig set is\n",
    "\\begin{equation}\n",
    "J(W, b) = -\\frac{1}{m}\\left[\\sum_{i=1}^{m}\\sum_{j=1}^{k}1\\{y^{(i)}=j\\}\\log\\frac{e^{w_j^Tx^{(i)}}}{\\sum_{l=1}^{k}e^{w_l^Tx^{(i)}}}\\right] + \\frac{\\lambda}{2}\\sum_{i=1}^{k}\\sum_{j=1}^{n}w_{ij}^2\n",
    "\\end{equation}\n",
    "\n",
    "The second term is a weight decay term to disambiguate $W$ and $b$ that could yeild the least training error.\n",
    "\n",
    "\n",
    "$J(W,b)$ is a convex function, and thus gradient descent will not run into a local optima problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the Model\n",
    "\n",
    "Gradient descent\n",
    "\n",
    "$$W \\leftarrow W - \\alpha \\frac{\\partial{J(W,b)}}{\\partial{W}}$$\n",
    "\n",
    "$$b \\leftarrow b - \\alpha \\frac{\\partial{J(W,b)}}{\\partial{b}}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "$$\\frac{\\partial{J(W,b)}}{\\partial{w_j}} = -\\frac{1}{m}\\sum_{i=1}^{m}\\left[x^{(i)}(1\\{y^{(i)}\\} - p(y^{(i)}=j|x^{(i)};W,b))\\right] + \\lambda w_j$$\n",
    "\n",
    "$$\\frac{\\partial{J(W,b)}}{\\partial{b}} = -\\frac{1}{m}\\sum_{i=1}^{m}\\left(1\\{y^{(i)}\\} - p(y^{(i)}=j|x^{(i)};W,b)\\right)$$\n",
    "\n",
    "A more advanced option is the L-BFGS alogrithm, which also requires the gradient function as an input argument.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model with Theano\n",
    "\n",
    "#### Vectorising the model\n",
    "\n",
    "Notations\n",
    "* X: data matrix of size $n\\times m$, where $n$ is the number of dimensions and m the number of instances. That is, each column stores an instance.\n",
    "* W: weight matrix of size $k \\times n$, where $k$ is the number of classes.\n",
    "* b: bias vector of size $k\\times 1$.\n",
    "\n",
    "The posterior probability is\n",
    "\\begin{equation}\n",
    "p = softmax(WX+b)\n",
    "\\end{equation}\n",
    "where softmax(M) = M / M.sum(axis=0) and M is a arbitrary matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities for manipulating the paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_size(shape):\n",
    "    \"\"\"\n",
    "    count the number of elements in a ndarray with shape=shape\n",
    "    \"\"\"\n",
    "    size = 1\n",
    "    for i in shape:\n",
    "        size *= i\n",
    "    return size\n",
    "    \n",
    "def pack(param_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        param_list: list of ndarrays\n",
    "    Returns:\n",
    "        theta:  vector of shape (?,), flattened params\n",
    "        shapes: list of tuples, with each tuple storing the shape of each ndarray in param_list\n",
    "    \"\"\"\n",
    "    shapes = []\n",
    "    theta=None\n",
    "    for p in param_list:\n",
    "        size = p.size\n",
    "        p2 = p.reshape((size, )) \n",
    "        if theta is None:\n",
    "            theta = p2\n",
    "        else:\n",
    "            theta = np.hstack((theta, p2))\n",
    "        shapes += [p.shape] \n",
    "    return theta, shapes\n",
    "        \n",
    "def unpack(theta, shapes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        theta:  vector of shape (?,), flattened params\n",
    "        shapes: list of tuples, with each tuple storing the shape of each ndarray in param_list      \n",
    "    Returns:\n",
    "        param_list: list of ndarrays\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    params = []\n",
    "    for shape in shapes:\n",
    "        size = get_size(shape)\n",
    "        x = theta[i:i+size]\n",
    "        params += [x.reshape(shape)]\n",
    "        i += size\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the softmax regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import gzip\n",
    "import cPickle\n",
    "\n",
    "#from param_util import pack, unpack\n",
    "\n",
    "# for debugging\n",
    "theano.config.optimizer=\"fast_run\"\n",
    "theano.config.exception_verbosity=\"high\"\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "    def __init__(self, n_in, n_out, L2_reg_coef, max_iter=100):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.L2_reg_coef = L2_reg_coef\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        self.W = theano.shared(\n",
    "            value=0.005 * np.random.randn(self.n_out, self.n_in),\n",
    "            name='W',\n",
    "            borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            value=np.zeros((self.n_out, 1), dtype=theano.config.floatX),\n",
    "            name='b',\n",
    "            broadcastable=(False, True),\n",
    "            borrow=True)\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "        # sample matrix, each column stores a sample\n",
    "        X = T.dmatrix('X')\n",
    "        # label\n",
    "        y = T.lvector('y')\n",
    "        # predict\n",
    "        p_y_given_x = self.__softmax__(T.dot(self.W, X) + self.b)\n",
    "        pred = T.argmax(p_y_given_x, axis=0)\n",
    "        # NLL: negative log-likelihood\n",
    "        nll = -T.mean(T.log(p_y_given_x[y, T.arange(0, y.shape[0])]))\n",
    "        # cost (loss)\n",
    "        cost = nll + self.L2_reg_coef * (self.W**2).sum()\n",
    "        # error rate\n",
    "        error = 1.0*T.sum(T.neq(pred, y))/X.shape[1]\n",
    "        \n",
    "        self.test_model = theano.function(\n",
    "            inputs=[X, y],\n",
    "            outputs=error)\n",
    "        self.run_model = theano.function(\n",
    "            inputs=[X],\n",
    "            outputs=pred)\n",
    "            \n",
    "        # compute gradient\n",
    "        grads = T.grad(cost, self.params)\n",
    "        params = [p.type() for p in self.params]\n",
    "        self.grad_func = theano.function(\n",
    "            inputs=params + [X, y],\n",
    "            outputs=grads,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "        self.cost_func = theano.function(\n",
    "            inputs=params + [X, y],\n",
    "            outputs=cost,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])   \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        init_theta, shapes = pack([self.W.get_value(), self.b.get_value()])\n",
    "        opt_theta, opt_cost, d = sp.optimize.fmin_l_bfgs_b(\n",
    "            func=self.__cost_and_grad__,\n",
    "            x0=init_theta,\n",
    "            fprime=None,\n",
    "            args=(shapes, X, y),\n",
    "            maxiter=self.max_iter,\n",
    "            iprint=1)\n",
    "        opt_param_list = unpack(opt_theta, shapes)\n",
    "        for shared_param, opt_param in zip(self.params, opt_param_list):\n",
    "             shared_param.set_value(opt_param)      \n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.run_model(X)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        return self.test_model(X, y)\n",
    "        \n",
    "    def __cost_and_grad__(self, theta, *args):\n",
    "        shapes = args[0]\n",
    "        X = args[1]\n",
    "        y = args[2]\n",
    "        param_list = unpack(theta, shapes)\n",
    "        cost =  self.cost_func(*(param_list + [X, y]))\n",
    "        grad = self.grad_func(*(param_list + [X, y]))\n",
    "        grad, _ = pack(grad)\n",
    "        return cost, grad        \n",
    "    \n",
    "    def __softmax__(self, M):\n",
    "        \"\"\"\n",
    "        normalise along the vertical axis\n",
    "        \"\"\"\n",
    "        e_M = T.exp(M - M.max(axis=0, keepdims=True))\n",
    "        return e_M / e_M.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model\n",
    "\n",
    "##### Dataset: MNIST Dataset of Handwritten Digits\n",
    "\n",
    "* Gray-scale image of size $28\\times 28$ with value ranging from [0, 1].\n",
    "* 50,000 training samples, 10,000 validation samples and 10,000 testing samples.\n",
    "\n",
    "![image](mnist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 training samples of dim 784\n",
      "error rate on test set is 8.5%, accuracy is 91.5%\n",
      "baseline: error rate on test set is 90.18, accuracy is 9.82%%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    f = gzip.open('data/mnist.pkl.gz')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    \n",
    "    # train model\n",
    "    train_X = train_set[0].transpose()\n",
    "    train_y = train_set[1]\n",
    "    valid_X = valid_set[0].transpose()\n",
    "    valid_y = valid_set[1]\n",
    "    \n",
    "    train_X = np.hstack((train_X, valid_X))\n",
    "    train_y = np.hstack((train_y, valid_y))\n",
    "    \n",
    "    train_X = train_X[:, 0:60001:5]\n",
    "    train_y = train_y[0:60001:5]\n",
    "    \n",
    "    n_in = train_X.shape[0]\n",
    "    n_out = np.unique(train_y).shape[0]\n",
    "    print \"{n} training samples of dim {d}\".format(n=train_X.shape[1], d=n_in)\n",
    "    \n",
    "    softmax_regression = SoftmaxRegression(n_in, n_out, L2_reg_coef=1e-4, max_iter=100)\n",
    "    softmax_regression.fit(train_X, train_y)\n",
    "    \n",
    "    # test model\n",
    "    test_X = test_set[0].transpose()\n",
    "    test_y = test_set[1]\n",
    "    error = softmax_regression.evaluate(test_X, test_y)\n",
    "    print 'error rate on test set is {e}%, accuracy is {a}%'.format(e=error*100, a=100-100*error)\n",
    "    # baseline method\n",
    "    pred_baseline = np.random.randint(\n",
    "        low = np.min(train_y),\n",
    "        high = np.max(train_y)+1,\n",
    "        size=valid_y.shape)\n",
    "    error_baseline = 1.0 * (pred_baseline != valid_y).sum() / valid_X.shape[1]\n",
    "    print 'baseline: error rate on test set is {e}, accuracy is {a}%'.format(e=error_baseline*100, a=100-100*error_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualising the most \"preferable\" input\n",
    "\n",
    "(TODO)拉格朗日乘子法https://en.wikipedia.org/wiki/Lagrange_multiplier, KKT条件等http://blog.csdn.net/xianlingmao/article/details/7919597"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highlights of DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "Suppose we have only a set of unlabeled training examples $\\{x^{(1)},...,x^{(m)}\\}$, where $x^{(i)}\\in R^n$.\n",
    "An autoencoder neural network is an unsupervised learning algorithm that applies backpropagation, setting the target values to be equal to the inputs, i.e., $y^{(i)}=x^{(i)}$.\n",
    "\n",
    "![image](autoencoder.png)\n",
    "\n",
    "The autoencoder tries to learn an identity function $h_{W,b}\\approx x$.\n",
    "* The identity function seems a particularly trivial function to be trying to learn;\n",
    "* By placing constraints on the network, such as by limiting the number of hidden units, we can discover interesting structure about the data.\n",
    "* As a concrete example, suppose the inputs $x$ are the pixel intensity values from a $10\\times 10$ image ($100$ pixels) so $n=100$, and there are $s_2=50$ hidden units in layer $L_2$. \n",
    "Since there are only $50$ hidden units, the network is forced to learn a compressed representation of the input.\n",
    "* If the inputs are completely random, i.e., each dimension comes from an independent distribution, the autoencoding task would be very difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notations\n",
    "* $m$: the number of samples\n",
    "* $n_l$: the number of layers, including the input, hidden and output layers\n",
    "* $s_l$: the number of units in layer $L_l$, excluding the bias unit\n",
    "* $W^{(l)}$: the weight matrix connecting layer $L_l$ and layer $L_{l+1}$\n",
    "* $W_{ji}^{(l)}$: the weight connecting unit $i$ in layer $L_l$ and unit $j$ in layer $L_{l+1}$\n",
    "* $b^{(l)}$: the bias vector connecting the bias unit in layer $L_l$ to the units in layer $L_{l+1}$\n",
    "* $z^{(l)}_j$: the linear input of the unit $j$ in layer $L_l$\n",
    "* $a^{(l)}_j$: the activation of the unit $j$ in layer $L_l$\n",
    "\n",
    "Foward propagation\n",
    "$$z^{(2)} = W^{(1)}x + b^{(1)}$$\n",
    "\n",
    "$$a^{(2)} = \\text{sigmoid}(z^{(2)})$$\n",
    "\n",
    "$$z^{(3)} = W^{(2)}a^{(2)} + b^{(2)}$$\n",
    "\n",
    "$$a^{(3)} = \\text{sigmoid}(z^{(3)})$$\n",
    "\n",
    "$$h_{W,b}(x) = a^{(3)}(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Loss Function\n",
    "\n",
    "For a single training sample $x$, the loss function is defined as\n",
    "\\begin{equation}\n",
    "J(W,b; x) = \\frac{1}{2}\\Vert h_{W,b}(x)-x\\Vert^2\n",
    "\\end{equation}\n",
    "Given a trainig set of $m$ samoples, we define the loss function as\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "J(W,b) &= \n",
    "\\left[\\frac{1}{m}\\sum_{i=1}^{m}J(W,b; x^{(i)})\\right]\n",
    "+\\lambda \\sum_{l=1}^{n_{l-1}}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}\\left(W_{ji}^{(l)}\\right)^2\\\\\n",
    "&=\\left[\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\frac{1}{2}\\Vert h_{W,}(x^{(i)})-x^{(i)}\\Vert^2\\right)\\right]\n",
    "+\\lambda \\sum_{l=1}^{n_{l-1}}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}\\left(W_{ji}^{(l)}\\right)^2\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The second term is a weight decay term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity Constraint\n",
    "\n",
    "We would like to constrain the neurons to be active only for a subset of patterns.\n",
    "* $a^{(2)}_j(x)$ denotes the activiation of hidden unit $j$ when the network is given a specific input $x$.\n",
    "* The average activation of hidden unit $j$ over the training set is\n",
    "\\begin{equation}\n",
    "\\hat{\\rho}_j = \\frac{1}{m}\\sum_{i=1}^{m}\\left[a^{(2)}_j(x)\\right]\n",
    "\\end{equation}\n",
    "* We would like to enforce the constraint\n",
    "$$\\hat{\\rho}_j=\\rho$$\n",
    "where $\\rho$ is a sparsity parameter, typically a small value close to zero.\n",
    "* A penalty term that penalises $\\hat{\\rho}_j$ deviating significantly from $\\rho$\n",
    "\\begin{equation}\n",
    "\\sum_{j=1}^{s_2}KL(\\rho||\\hat{\\rho}_j) = \\sum_{j=1}^{s_2}\\left[\\rho\\log\\frac{\\rho}{\\hat{\\rho}_j} + (1-\\rho)\\log\\frac{1-\\rho}{1-\\hat{\\rho}_j}\\right]\n",
    "\\end{equation}\n",
    "    * $KL(\\rho||\\hat{\\rho}_j)$ has the property that $KL(\\rho||\\hat{\\rho}_j) = 0$ if $\\hat{\\rho}_j=\\rho$, and otherwise it increases monotonically as $\\hat{\\rho}_j$ diverges from $\\rho$.\n",
    "    * In the figure below, we set $\\rho = 0.2$.\n",
    "\n",
    "![image](kl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our overall loss function for sparse autoencoder is\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "J(W,b) &= \n",
    "\\left[\\frac{1}{m}\\sum_{i=1}^{m}J(W,b; x^{(i)})\\right]\n",
    "+\\lambda \\sum_{l=1}^{n_{l-1}}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}\\left(W_{ji}^{(l)}\\right)^2 + \\beta\\sum_{j=1}^{s_2}KL(\\rho||\\hat{\\rho}_j)\\\\\n",
    "&=\\left[\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\frac{1}{2}\\Vert h_{W,b}(x^{(i)})-x^{(i)}\\Vert^2\\right)\\right]\n",
    "+\\lambda \\sum_{l=1}^{n_{l-1}}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}\\left(W_{ji}^{(l)}\\right)^2 + \\beta \\sum_{j=1}^{s_2}\\left[\\rho\\log\\frac{\\rho}{\\hat{\\rho}_j} + (1-\\rho)\\log\\frac{1-\\rho}{1-\\hat{\\rho}_j}\\right]\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model with Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities for paramter manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import numpy as np\n",
    "\n",
    "def get_size(shape):\n",
    "    \"\"\"\n",
    "    count the number of elements in a ndarray with shape=shape\n",
    "    \"\"\"\n",
    "    size = 1\n",
    "    for i in shape:\n",
    "        size *= i\n",
    "    return size\n",
    "    \n",
    "def pack(param_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        param_list: list of ndarrays\n",
    "    Returns:\n",
    "        theta:  vector of shape (?,), flattened params\n",
    "        shapes: list of tuples, with each tuple storing the shape of each ndarray in param_list\n",
    "    \"\"\"\n",
    "    shapes = []\n",
    "    theta=None\n",
    "    for p in param_list:\n",
    "        size = p.size\n",
    "        p2 = p.reshape((size, )) \n",
    "        if theta is None:\n",
    "            theta = p2\n",
    "        else:\n",
    "            theta = np.hstack((theta, p2))\n",
    "        shapes += [p.shape] \n",
    "    return theta, shapes\n",
    "        \n",
    "def unpack(theta, shapes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        theta:  vector of shape (?,), flattened params\n",
    "        shapes: list of tuples, with each tuple storing the shape of each ndarray in param_list      \n",
    "    Returns:\n",
    "        param_list: list of ndarrays\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    params = []\n",
    "    for shape in shapes:\n",
    "        size = get_size(shape)\n",
    "        x = theta[i:i+size]\n",
    "        params += [x.reshape(shape)]\n",
    "        i += size\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, n_in, n_out, name=''):\n",
    "        r = -np.sqrt(6.0 / (n_in + n_out + 1))\n",
    "        rand_W = 2 * r * np.random.rand(n_out, n_in) - r\n",
    "        self.W = theano.shared(\n",
    "            value=rand_W,\n",
    "            name='W_'+name,\n",
    "            borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            value=np.zeros((n_out,1), dtype=theano.config.floatX),\n",
    "            name='b_'+name,\n",
    "            borrow=True,\n",
    "            broadcastable=(False,True))\n",
    "        self.params = [self.W, self.b]\n",
    "        self.L2_sqr = (self.W ** 2).sum()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: matrix tensor, each column stores a sample\n",
    "        \"\"\"\n",
    "        activation = T.nnet.sigmoid(T.dot(self.W, X) + self.b)\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a AutoEncoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import gzip\n",
    "import cPickle\n",
    "\n",
    "#from param_util import pack, unpack\n",
    "#from layer import Layer\n",
    "\n",
    "# for debugging\n",
    "theano.config.optimizer='fast_run'#'fast_compile'\n",
    "theano.config.exception_verbosity=\"high\"\n",
    "\n",
    "class AutoEncoder(object):\n",
    "    def __init__(self, n_in, n_hid, \n",
    "                 L2_reg_coef, sparse_reg_coef, sparse_rho, \n",
    "                 max_iter=400):\n",
    "        self.n_in = n_in\n",
    "        self.n_hid = n_hid\n",
    "        self.L2_reg_coef = L2_reg_coef\n",
    "        self.sparse_reg_coef = sparse_reg_coef\n",
    "        self.sparse_rho = sparse_rho\n",
    "        self.max_iter = max_iter \n",
    "        \n",
    "        self.hidden_layer = Layer(self.n_in, self.n_hid, 'hidden')\n",
    "        self.output_layer = Layer(self.n_hid, self.n_in, 'output')\n",
    "        self.params = self.hidden_layer.params + self.output_layer.params\n",
    "        \n",
    "        X = T.dmatrix(name='X') # each column stores a sample\n",
    "        m = X.shape[1] # sample count\n",
    "        activation = self.hidden_layer.forward(X)\n",
    "        output = self.output_layer.forward(activation)\n",
    "        error = ((output - X) ** 2).sum() / (2.0 * m)\n",
    "        L2_reg = self.hidden_layer.L2_sqr + self.output_layer.L2_sqr\n",
    "        rho = activation.mean(axis=1)\n",
    "        kl = (self.sparse_rho * T.log(self.sparse_rho/rho) + \n",
    "              (1-self.sparse_rho) * T.log((1-self.sparse_rho)/(1-rho))).sum()\n",
    "        cost = error + self.L2_reg_coef * 0.5 * L2_reg + self.sparse_reg_coef * kl\n",
    "        grads = T.grad(cost, self.params)\n",
    "        params = [p.type() for p in self.params]\n",
    "        self.cost_func = theano.function(\n",
    "            inputs=params + [X],\n",
    "            outputs=cost,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "        self.grad_func = theano.function(\n",
    "            inputs=params + [X],\n",
    "            outputs=grads,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "        self.run_model = theano.function(\n",
    "            inputs=[X],\n",
    "            outputs=activation)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: matrix tensor, each column stores a sample\n",
    "        \"\"\"\n",
    "        return self.hidden_layer.forward(X)\n",
    "            \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        X: np.ndarray, each column stores a sample\n",
    "        \"\"\"\n",
    "        init_theta, shapes = pack([\n",
    "            self.hidden_layer.W.get_value(), self.hidden_layer.b.get_value(), \n",
    "            self.output_layer.W.get_value(), self.output_layer.b.get_value()])\n",
    "        opt_theta, opt_cost, d = sp.optimize.fmin_l_bfgs_b(\n",
    "            func=self.cost_and_grad,\n",
    "            x0=init_theta,\n",
    "            fprime=None,\n",
    "            args=(shapes, X),\n",
    "            maxiter=self.max_iter,\n",
    "            iprint=1)\n",
    "        opt_param_list = unpack(opt_theta, shapes)\n",
    "        for shared_param, opt_param in zip(self.params, opt_param_list):\n",
    "             shared_param.set_value(opt_param)\n",
    "             \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        X: np.ndarray, each column stores a sample\n",
    "        \"\"\"\n",
    "        return self.run_model(X)\n",
    "\n",
    "    def cost_and_grad(self, theta, *args):\n",
    "        \"\"\"\n",
    "        compute the cost and gradient at theta in the parameter space\n",
    "        args should be (shapes, X)\n",
    "        X: each column stores a sample\n",
    "        \"\"\"\n",
    "        shapes = args[0]\n",
    "        X = args[1]\n",
    "        param_list = unpack(theta, shapes)\n",
    "        cost =  self.cost_func(*(param_list+[X]))\n",
    "        grad = self.grad_func(*(param_list+[X]))\n",
    "        grad, _ = pack(grad)\n",
    "        return cost, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 samples of dimension 784\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2209594d4c97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0msparse_rho\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         max_iter=200)\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mauto_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-90afefb49995>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             iprint=1)\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mopt_param_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mshared_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_param\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_param_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/taiqing/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/optimize/lbfgsb.pyc\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0;32m--> 186\u001b[0;31m                            **opts)\n\u001b[0m\u001b[1;32m    187\u001b[0m     d = {'grad': res['jac'],\n\u001b[1;32m    188\u001b[0m          \u001b[0;34m'task'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/taiqing/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/optimize/lbfgsb.pyc\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, **unknown_options)\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0;31m# minimization routine wants f and g at the current x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/taiqing/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/optimize/lbfgsb.pyc\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/taiqing/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/optimize/optimize.pyc\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/taiqing/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/optimize/optimize.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-90afefb49995>\u001b[0m in \u001b[0;36mcost_and_grad\u001b[0;34m(self, theta, *args)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mparam_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_list\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_list\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/taiqing/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    plt.close('all')\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    #X = cPickle.load(open('data/patches', 'r'))\n",
    "    f = gzip.open('data/mnist.pkl.gz')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    train_X = train_set[0].transpose()\n",
    "    X = train_X[:, 0:50001:5]\n",
    "    print '{m} samples of dimension {d}'.format(m=X.shape[1], d=X.shape[0])\n",
    "    auto_encoder = AutoEncoder(\n",
    "        n_in=X.shape[0],\n",
    "        n_hid = 200,\n",
    "        L2_reg_coef=3e-3,\n",
    "        sparse_reg_coef=3.0,\n",
    "        sparse_rho=0.1,\n",
    "        max_iter=200)\n",
    "    auto_encoder.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising a Trained Autoencoder\n",
    "\n",
    "Given a specific input $x$, the activation of hidden unit $i$ is\n",
    "$$a^{(2)}_i(x) = \\text{sigmoid}(\\sum_{j=1}^{s_1}W_{ij}^{(1)}x_j + b_i)$$\n",
    "\n",
    "Find the input $x^*$ that maximises $a^{(2)}_i$, i.e., causes hidden unit $i$ to be maximally activated.\n",
    "\n",
    "Considering\n",
    "* Sigmoid function is a monotonically increasing function;\n",
    "* The input should be constrained to have a limited magnitude,\n",
    "\n",
    "we have\n",
    "\\begin{equation}\n",
    "x^* = \\arg\\max_{x}{\\sum_{j=1}^{s_1}W_{ij}^{(1)}x_j + b_i}\\\\\n",
    "s.t. \\Vert x \\Vert^2 \\le 1\n",
    "\\end{equation}\n",
    "\n",
    "** Lemma 1** The optimal $x^*$ has identity norm, that is\n",
    "$$\\Vert x^* \\Vert^2 = 1$$\n",
    "\n",
    "Solve for $x^*$ by lagrange multiplier:\n",
    "\\begin{equation}\n",
    "F(x, \\lambda) = {\\sum_{j=1}^{s_1}W_{ij}^{(1)}x_j + b_i} + \\lambda \\left(\\sum_{j=1}^{s_1}x_j^2-1\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Setting\n",
    "\\begin{equation}\n",
    "\\frac{\\partial F(x, \\lambda)}{\\partial x_j} = W_{ij}^{(1)}x_j + 2\\lambda x_j = 0\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial F(x, \\lambda)}{\\partial \\lambda} = \\sum_{j=1}^{s_1}x_j^2 - 1 = 0\n",
    "\\end{equation}\n",
    "\n",
    "By solving the above equations, we have\n",
    "\\begin{equation}\n",
    "x_j^* = \\frac{W_{ij}^{(1)}}{\\sqrt{\\sum_{j=1}^{s_1}\\left(W_{ij}^{(1)}\\right)^2}}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(0)\n",
    "n = 8\n",
    "for i in np.arange(n**2):\n",
    "    ax = fig.add_subplot(n, n, i+1)\n",
    "    w = auto_encoder.hidden_layer.W.get_value()[i, :]\n",
    "    w = w / np.sqrt((w ** 2).sum())\n",
    "    w = (w - w.min()) / (w.max()-w.min())\n",
    "    d = np.floor(np.sqrt(w.shape[0]))\n",
    "    ax.imshow(w.reshape((d,d)), cmap='gray', interpolation='none')\n",
    "    ax.set_axis_off()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](visual_autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Deep Networks for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
