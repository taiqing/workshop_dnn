{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a Taste of DNN on the Shoulder of Theano\n",
    "===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](fruit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "* Introduction to Theano\n",
    "* Softmax regression\n",
    "* Highlights of DNN\n",
    "* Neural network\n",
    "    * Multiple layer perception\n",
    "    * Forward propagation\n",
    "    * Backward propagation\n",
    "* Sparse autoencoder\n",
    "    * Autoencoder\n",
    "    * Sparse autoencoder\n",
    "* Building deep networks for classification\n",
    "    * Model structure\n",
    "    * Pre-training\n",
    "    * Fine-tuning\n",
    "    * Experimental results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Theano\n",
    "Theano is Python library that allows you to define, evaluate and optimize math expressions.\n",
    "* Efficient symbolic differentiation\n",
    "* Efficient handling of matrices\n",
    "* Tight integration of NumPy\n",
    "* Dynamic C code generate\n",
    "* Transparent use of GPU\n",
    "\n",
    "#### Fast to develop and fast to run\n",
    "![image](fast.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine learning libraries built on top of Theano:\n",
    "* Pylearn2\n",
    "    * great flexibility and a good choice for trying out ML ideas\n",
    "* PyMC3\n",
    "    * Probabilistic programming; building statistical Bayesian models\n",
    "* Sklearn-theano\n",
    "    * Easy-to-use deep learning tool\n",
    "* Lasagne\n",
    "    * Lightweight library to build neural networks\n",
    "\n",
    "#### Models that have been built with Theano:\n",
    "* Neural networks\n",
    "* Convolutional Neural Networks (CNN)\n",
    "* Recurrent Neural Networks (RNN)\n",
    "* Long Short Term Memory (LSTM)\n",
    "* Autoencoders\n",
    "* GoogLeNet\n",
    "* Overfeat\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symbolic variables in Theano\n",
    "* Variable (C, Java, Python, etc.)\n",
    "    * A segment of physical storage in RAM\n",
    "    * Operations are based on value passing between variables\n",
    "* Tensor (Theano)\n",
    "    * A mathematical symbol\n",
    "    * No physical storage in RAM to hold its value\n",
    "    * Operations are actually building connections between tensors\n",
    "* Shared variable (Theano)\n",
    "    * Hybrid of variable and tensor\n",
    "    * Tensor with physical storage in RAM to hold its value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output file is available at 1.png\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "x = T.dvector(name='x')\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "y = f(x)\n",
    "\n",
    "theano.printing.pydotprint(theano.function([x], y), '1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_theano.function_ brings life to theano variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is [ 1.  2.  3.], b is [ 1.  4.  9.]\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "\n",
    "x = T.dvector(name='x')\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "y = f(x)\n",
    "\n",
    "pow2 = theano.function(inputs=[x], outputs=y)\n",
    "\n",
    "a = np.array([1,2,3], dtype=theano.config.floatX)\n",
    "b = pow2(a)\n",
    "print \"a is {a}, b is {b}\".format(a=a, b=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 1.,  1.,  1.])]\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "\n",
    "x = T.dvector('x')\n",
    "y = x.sum()\n",
    "grad = T.grad(cost=y, wrt=[x])\n",
    "grad_func = theano.function(inputs=[x], outputs=grad)\n",
    "\n",
    "a = np.array([1,2,3], dtype=theano.config.floatX)\n",
    "print grad_func(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "In the softmax regression setting, we are interested in multi-class classification. Suppose we have $m$ samples in the training set $\\{(x^{(1)}, y^{(1)}),...,(x^{(m)}, y^{(m)})\\}$, where $y^{(i)}\\in \\{1,2,...,k\\}$ and $x^{(i)}\\in R^{n}$.\n",
    "\n",
    "Given a test sample $x$, we want to estimate the probability that $x$ belongs to class $j$, i.e., $p(y=j|x)$, for all possible $j$.\n",
    "\n",
    "\\begin{equation}\n",
    "h_{W,b}(x) = \n",
    "\\left[\n",
    "  \\begin{array}{c}\n",
    "  p(y=1|x;W,b)\\\\\n",
    "  p(y=2|x;W,b)\\\\\n",
    "  ...\\\\\n",
    "  p(y=k|x;W,b)\\\\\n",
    "  \\end{array}\n",
    "\\right]\n",
    "=\\frac{1}{\\sum_{j=1}^{k}{e^{w_j^Tx+b_j}}}\n",
    "\\left[\n",
    "  \\begin{array}{c}\n",
    "  e^{w_1^Tx+b_1}\\\\\n",
    "  e^{w_2^Tx+b_2}\\\\\n",
    "  ...\\\\\n",
    "  e^{w_k^Tx+b_k}\\\\\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{equation}\n",
    "\n",
    "When you implement softmax regression, it is usually convenient to represent $W$ as a $n$-by-$k$ matrix, so that\n",
    "\\begin{equation}\n",
    "W = \\left[\\begin{array}{c}\n",
    "w_1^T\\\\\n",
    "...\\\\\n",
    "w_k^T\n",
    "\\end{array}\\right]\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Defining a Loss Function\n",
    "The loss of $h_{W, b}$ on the trainig set is\n",
    "\\begin{equation}\n",
    "J(W, b) = -\\frac{1}{m}\\left[\\sum_{i=1}^{m}\\sum_{j=1}^{k}1\\{y^{(i)}=j\\}\\log\\frac{e^{w_j^Tx^{(i)}}}{\\sum_{l=1}^{k}e^{w_l^Tx^{(i)}}}\\right] + \\frac{\\lambda}{2}\\sum_{i=1}^{k}\\sum_{j=1}^{n}w_{ij}^2\n",
    "\\end{equation}\n",
    "\n",
    "The second term is a weight decay term to disambiguate $W$ and $b$ that could yeild the least training error.\n",
    "\n",
    "\n",
    "$J(W,b)$ is a convex function, and thus gradient descent will not run into a local optima problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the Model\n",
    "\n",
    "Gradient descent\n",
    "\n",
    "$$W \\leftarrow W - \\alpha \\frac{\\partial{J(W,b)}}{\\partial{W}}$$\n",
    "\n",
    "$$b \\leftarrow b - \\alpha \\frac{\\partial{J(W,b)}}{\\partial{b}}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "$$\\frac{\\partial{J(W,b)}}{\\partial{w_j}} = -\\frac{1}{m}\\sum_{i=1}^{m}\\left[x^{(i)}(1\\{y^{(i)}\\} - p(y^{(i)}=j|x^{(i)};W,b))\\right] + \\lambda w_j$$\n",
    "\n",
    "$$\\frac{\\partial{J(W,b)}}{\\partial{b}} = -\\frac{1}{m}\\sum_{i=1}^{m}\\left(1\\{y^{(i)}\\} - p(y^{(i)}=j|x^{(i)};W,b)\\right)$$\n",
    "\n",
    "A more advanced option is the L-BFGS alogrithm, which also requires the gradient function as an input argument.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model with Theano\n",
    "\n",
    "#### Vectorising the model\n",
    "\n",
    "Notations\n",
    "* X: data matrix of size $n\\times m$, where $n$ is the number of dimensions and m the number of instances. That is, each column stores an instance.\n",
    "* W: weight matrix of size $k \\times n$, where $k$ is the number of classes.\n",
    "* b: bias vector of size $k\\times 1$.\n",
    "\n",
    "The posterior probability is\n",
    "\\begin{equation}\n",
    "p = softmax(WX+b)\n",
    "\\end{equation}\n",
    "where softmax(M) = M / M.sum(axis=0) and M is a arbitrary matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities for manipulating the paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_size(shape):\n",
    "    \"\"\"\n",
    "    count the number of elements in a ndarray with shape=shape\n",
    "    \"\"\"\n",
    "    size = 1\n",
    "    for i in shape:\n",
    "        size *= i\n",
    "    return size\n",
    "    \n",
    "def pack(param_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        param_list: list of ndarrays\n",
    "    Returns:\n",
    "        theta:  vector of shape (?,), flattened params\n",
    "        shapes: list of tuples, with each tuple storing the shape of each ndarray in param_list\n",
    "    \"\"\"\n",
    "    shapes = []\n",
    "    theta=None\n",
    "    for p in param_list:\n",
    "        size = p.size\n",
    "        p2 = p.reshape((size, )) \n",
    "        if theta is None:\n",
    "            theta = p2\n",
    "        else:\n",
    "            theta = np.hstack((theta, p2))\n",
    "        shapes += [p.shape] \n",
    "    return theta, shapes\n",
    "        \n",
    "def unpack(theta, shapes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        theta:  vector of shape (?,), flattened params\n",
    "        shapes: list of tuples, with each tuple storing the shape of each ndarray in param_list      \n",
    "    Returns:\n",
    "        param_list: list of ndarrays\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    params = []\n",
    "    for shape in shapes:\n",
    "        size = get_size(shape)\n",
    "        x = theta[i:i+size]\n",
    "        params += [x.reshape(shape)]\n",
    "        i += size\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the softmax regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import gzip\n",
    "import cPickle\n",
    "\n",
    "#from param_util import pack, unpack\n",
    "\n",
    "# for debugging\n",
    "theano.config.optimizer=\"fast_run\"\n",
    "theano.config.exception_verbosity=\"high\"\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "    def __init__(self, n_in, n_out, L2_reg_coef, max_iter=100):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.L2_reg_coef = L2_reg_coef\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        self.W = theano.shared(\n",
    "            value=0.005 * np.random.randn(self.n_out, self.n_in),\n",
    "            name='W',\n",
    "            borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            value=np.zeros((self.n_out, 1), dtype=theano.config.floatX),\n",
    "            name='b',\n",
    "            broadcastable=(False, True),\n",
    "            borrow=True)\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "        # sample matrix, each column stores a sample\n",
    "        X = T.dmatrix('X')\n",
    "        # label\n",
    "        y = T.lvector('y')\n",
    "        # predict\n",
    "        p_y_given_x = self.__softmax__(T.dot(self.W, X) + self.b)\n",
    "        pred = T.argmax(p_y_given_x, axis=0)\n",
    "        # NLL: negative log-likelihood\n",
    "        nll = -T.mean(T.log(p_y_given_x[y, T.arange(0, y.shape[0])]))\n",
    "        # cost (loss)\n",
    "        cost = nll + self.L2_reg_coef * (self.W**2).sum()\n",
    "        # error rate\n",
    "        error = 1.0*T.sum(T.neq(pred, y))/X.shape[1]\n",
    "        \n",
    "        self.test_model = theano.function(\n",
    "            inputs=[X, y],\n",
    "            outputs=error)\n",
    "        self.run_model = theano.function(\n",
    "            inputs=[X],\n",
    "            outputs=pred)\n",
    "            \n",
    "        # compute gradient\n",
    "        grads = T.grad(cost, self.params)\n",
    "        params = [p.type() for p in self.params]\n",
    "        self.grad_func = theano.function(\n",
    "            inputs=params + [X, y],\n",
    "            outputs=grads,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "        self.cost_func = theano.function(\n",
    "            inputs=params + [X, y],\n",
    "            outputs=cost,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])   \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        init_theta, shapes = pack([self.W.get_value(), self.b.get_value()])\n",
    "        opt_theta, opt_cost, d = sp.optimize.fmin_l_bfgs_b(\n",
    "            func=self.__cost_and_grad__,\n",
    "            x0=init_theta,\n",
    "            fprime=None,\n",
    "            args=(shapes, X, y),\n",
    "            maxiter=self.max_iter,\n",
    "            iprint=1)\n",
    "        opt_param_list = unpack(opt_theta, shapes)\n",
    "        for shared_param, opt_param in zip(self.params, opt_param_list):\n",
    "             shared_param.set_value(opt_param)      \n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.run_model(X)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        return self.test_model(X, y)\n",
    "        \n",
    "    def __cost_and_grad__(self, theta, *args):\n",
    "        shapes = args[0]\n",
    "        X = args[1]\n",
    "        y = args[2]\n",
    "        param_list = unpack(theta, shapes)\n",
    "        cost =  self.cost_func(*(param_list + [X, y]))\n",
    "        grad = self.grad_func(*(param_list + [X, y]))\n",
    "        grad, _ = pack(grad)\n",
    "        return cost, grad        \n",
    "    \n",
    "    def __softmax__(self, M):\n",
    "        \"\"\"\n",
    "        normalise along the vertical axis\n",
    "        \"\"\"\n",
    "        e_M = T.exp(M - M.max(axis=0, keepdims=True))\n",
    "        return e_M / e_M.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model\n",
    "\n",
    "##### Dataset: MNIST Dataset of Handwritten Digits\n",
    "\n",
    "* Gray-scale image of size $28\\times 28$ with value ranging from [0, 1].\n",
    "* 50,000 training samples, 10,000 validation samples and 10,000 testing samples.\n",
    "\n",
    "![image](mnist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 training samples of dim 784\n",
      "error rate on test set is 8.5%, accuracy is 91.5%\n",
      "baseline: error rate on test set is 90.18, accuracy is 9.82%%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    f = gzip.open('data/mnist.pkl.gz')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    \n",
    "    # train model\n",
    "    train_X = train_set[0].transpose()\n",
    "    train_y = train_set[1]\n",
    "    valid_X = valid_set[0].transpose()\n",
    "    valid_y = valid_set[1]\n",
    "    \n",
    "    train_X = np.hstack((train_X, valid_X))\n",
    "    train_y = np.hstack((train_y, valid_y))\n",
    "    \n",
    "    train_X = train_X[:, 0:60001:5]\n",
    "    train_y = train_y[0:60001:5]\n",
    "    \n",
    "    n_in = train_X.shape[0]\n",
    "    n_out = np.unique(train_y).shape[0]\n",
    "    print \"{n} training samples of dim {d}\".format(n=train_X.shape[1], d=n_in)\n",
    "    \n",
    "    softmax_regression = SoftmaxRegression(n_in, n_out, L2_reg_coef=1e-4, max_iter=100)\n",
    "    softmax_regression.fit(train_X, train_y)\n",
    "    \n",
    "    # test model\n",
    "    test_X = test_set[0].transpose()\n",
    "    test_y = test_set[1]\n",
    "    error = softmax_regression.evaluate(test_X, test_y)\n",
    "    print 'error rate on test set is {e}%, accuracy is {a}%'.format(e=error*100, a=100-100*error)\n",
    "    # baseline method\n",
    "    pred_baseline = np.random.randint(\n",
    "        low = np.min(train_y),\n",
    "        high = np.max(train_y)+1,\n",
    "        size=valid_y.shape)\n",
    "    error_baseline = 1.0 * (pred_baseline != valid_y).sum() / valid_X.shape[1]\n",
    "    print 'baseline: error rate on test set is {e}, accuracy is {a}%'.format(e=error_baseline*100, a=100-100*error_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualising the most \"preferable\" input\n",
    "\n",
    "(TODO)拉格朗日乘子法https://en.wikipedia.org/wiki/Lagrange_multiplier, KKT条件等http://blog.csdn.net/xianlingmao/article/details/7919597"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highlights of DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "Suppose we have only a set of unlabeled training examples $\\{x^{(1)},...,x^{(m)}\\}$, where $x^{(i)}\\in R^n$.\n",
    "An autoencoder neural network is an unsupervised learning algorithm that applies backpropagation, setting the target values to be equal to the inputs, i.e., $y^{(i)}=x^{(i)}$.\n",
    "\n",
    "![image](autoencoder.png)\n",
    "\n",
    "The autoencoder tries to learn an identity function $h_{W,b}\\approx x$. \n",
    "* The identity function seems a particularly trivial function to be trying to learn; but by placing constraints on the network, such as by limiting the number of hidden units, we can discover interesting structure about the data.\n",
    "* As a concrete example, suppose the inputs $x$ are the pixel intensity values from a $10\\times 10$ image ($100$ pixels) so $n=100$, and there are $s_2=50$ hidden units in layer $L_2$. \n",
    "Since there are only $50$ hidden units, the network is forced to learn a compressed representation of the input.\n",
    "* If the inputs are completely random, i.e., each dimension comes from an independent distribution, the autoencoding task would be very difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notations\n",
    "* $m$: the number of samples\n",
    "* $n_l$: the number of layers, including the input, hidden and output layers\n",
    "* $s_l$: the number of units in layer $L_l$, excluding the bias unit\n",
    "* $W^{(l)}$: the weight matrix connecting layer $L_l$ and layer $L_{l+1}$\n",
    "* $W_{ji}^{(l)}$: the weight connecting unit $i$ in layer $L_l$ and unit $j$ in layer $L_{l+1}$\n",
    "* $b^{(l)}$: the bias vector connecting the bias unit in layer $L_l$ to the units in layer $L_{l+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Loss Function\n",
    "\n",
    "For a single training sample $x$, the loss function is defined as\n",
    "\\begin{equation}\n",
    "J(W,b; x) = \\frac{1}{2}\\Vert h_{W,b}(x)-x\\Vert^2\n",
    "\\end{equation}\n",
    "Given a trainig set of $m$ samoples, we define the loss function as\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "J(W,b) &= \n",
    "\\left[\\frac{1}{m}\\sum_{i=1}^{m}J(W,b; x^{(i)})\\right]\n",
    "+\\lambda \\sum_{l=1}^{n_{l-1}}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}\\left(W_{ji}^{(l)}\\right)^2\\\\\n",
    "&=\\left[\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\frac{1}{2}\\Vert h_{W,}(x^{(i)})-x^{(i)}\\Vert^2\\right)\\right]\n",
    "+\\lambda \\sum_{l=1}^{n_{l-1}}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}\\left(W_{ji}^{(l)}\\right)^2\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The second term is a weight decay term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity Constraint\n",
    "\n",
    "We would like to constrain the neurons to be active only for a subset of patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
