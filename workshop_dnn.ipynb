{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Have a Taste of DNN on the Shoulder of Theano\n",
    "===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image](fruit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "* Introduction to Theano\n",
    "* Softmax regression\n",
    "* Highlights of DNN\n",
    "* Neural network\n",
    "    * Multiple layer perception\n",
    "    * Forward propagation\n",
    "    * Backward propagation\n",
    "* Sparse autoencoder\n",
    "    * Autoencoder\n",
    "    * Sparse autoencoder\n",
    "* Building deep networks for classification\n",
    "    * Model structure\n",
    "    * Pre-training\n",
    "    * Fine-tuning\n",
    "    * Experimental results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Theano\n",
    "Theano is Python library that allows you to define, evaluate and optimize math expressions.\n",
    "* Efficient symbolic differentiation\n",
    "* Efficient handling of matrices\n",
    "* Tight integration of NumPy\n",
    "* Dynamic C code generate\n",
    "* Transparent use of GPU\n",
    "\n",
    "#### Fast to develop and fast to run\n",
    "![image](fast.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine learning libraries built on top of Theano:\n",
    "* Pylearn2\n",
    "    * great flexibility and a good choice for trying out ML ideas\n",
    "* PyMC3\n",
    "    * Probabilistic programming; building statistical Bayesian models\n",
    "* Sklearn-theano\n",
    "    * Easy-to-use deep learning tool\n",
    "* Lasagne\n",
    "    * Lightweight library to build neural networks\n",
    "\n",
    "#### Models that have been built with Theano:\n",
    "* Neural networks\n",
    "* Convolutional Neural Networks (CNN)\n",
    "* Recurrent Neural Networks (RNN)\n",
    "* Long Short Term Memory (LSTM)\n",
    "* Autoencoders\n",
    "* GoogLeNet\n",
    "* Overfeat\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symbolic variables in Theano\n",
    "* Variable (C, Java, Python, etc.)\n",
    "    * A segment of physical storage in RAM\n",
    "    * Operations are based on value passing between variables\n",
    "* Tensor (Theano)\n",
    "    * A mathematical symbol\n",
    "    * No physical storage in RAM to hold its value\n",
    "    * Operations are actually building connections between tensors\n",
    "* Shared variable (Theano)\n",
    "    * Hybrid of variable and tensor\n",
    "    * Tensor with physical storage in RAM to hold its value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "x = T.dvector(name='x')\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "y = f(x)\n",
    "\n",
    "theano.printing.pydotprint(theano.function([x], y), '1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_theano.function_ brings life to theano variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "\n",
    "x = T.dvector(name='x')\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "y = f(x)\n",
    "\n",
    "pow2 = theano.function(inputs=[x], outputs=y)\n",
    "\n",
    "a = np.array([1,2,3], dtype=theano.config.floatX)\n",
    "b = pow2(a)\n",
    "print \"a is {a}, b is {b}\".format(a=a, b=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "\n",
    "x = T.dvector('x')\n",
    "y = x.sum()\n",
    "grad = T.grad(cost=y, wrt=[x])\n",
    "grad_func = theano.function(inputs=[x], outputs=grad)\n",
    "\n",
    "a = np.array([1,2,3], dtype=theano.config.floatX)\n",
    "print grad_func(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "In the softmax regression setting, we are interested in multi-class classification. Suppose we have $m$ samples in the training set $\\{(x^{(1)}, y^{(1)}),...,(x^{(m)}, y^{(m)})\\}$, where $y^{(i)}\\in \\{1,2,...,k\\}$ and $x^{(i)}\\in R^{n}$.\n",
    "\n",
    "Given a test sample $x$, we want to estimate the probability that $x$ belongs to class $j$, i.e., $p(y=j|x)$, for all possible $j$.\n",
    "\n",
    "\\begin{equation}\n",
    "h_{W,b}(x) = \n",
    "\\left[\n",
    "  \\begin{array}{c}\n",
    "  p(y=1|x;W,b)\\\\\n",
    "  p(y=2|x;W,b)\\\\\n",
    "  ...\\\\\n",
    "  p(y=k|x;W,b)\\\\\n",
    "  \\end{array}\n",
    "\\right]\n",
    "=\\frac{1}{\\sum_{j=1}^{k}{e^{w_j^Tx+b_j}}}\n",
    "\\left[\n",
    "  \\begin{array}{c}\n",
    "  e^{w_1^Tx+b_1}\\\\\n",
    "  e^{w_2^Tx+b_2}\\\\\n",
    "  ...\\\\\n",
    "  e^{w_k^Tx+b_k}\\\\\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{equation}\n",
    "\n",
    "When you implement softmax regression, it is usually convenient to represent $W$ as a $n$-by-$k$ matrix, so that\n",
    "\\begin{equation}\n",
    "W = \\left[\\begin{array}{c}\n",
    "w_1^T\\\\\n",
    "...\\\\\n",
    "w_k^T\n",
    "\\end{array}\\right]\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Defining a Loss Function\n",
    "The loss of $h_{W, b}$ on the trainig set is\n",
    "\\begin{equation}\n",
    "J(W, b) = -\\frac{1}{m}\\left[\\sum_{i=1}^{m}\\sum_{j=1}^{k}1\\{y^{(i)}=j\\}\\log\\frac{e^{w_j^Tx^{(i)}}}{\\sum_{l=1}^{k}e^{w_l^Tx^{(i)}}}\\right] + \\frac{\\lambda}{2}\\sum_{i=1}^{k}\\sum_{j=1}^{n}w_{ij}^2\n",
    "\\end{equation}\n",
    "\n",
    "The second term is a weight decay term to disambiguate $W$ and $b$ that could yeild the least training error.\n",
    "\n",
    "\n",
    "$J(W,b)$ is a convex function, and thus gradient descent will not run into a local optima problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the Model\n",
    "\n",
    "Gradient descent\n",
    "\n",
    "$$W \\leftarrow W - \\alpha \\frac{\\partial{J(W,b)}}{\\partial{W}}$$\n",
    "\n",
    "$$b \\leftarrow b - \\alpha \\frac{\\partial{J(W,b)}}{\\partial{b}}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "$$\\frac{\\partial{J(W,b)}}{\\partial{w_j}} = -\\frac{1}{m}\\sum_{i=1}^{m}\\left[x^{(i)}(1\\{y^{(i)}\\} - p(y^{(i)}=j|x^{(i)};W,b))\\right] + \\lambda w_j$$\n",
    "\n",
    "$$\\frac{\\partial{J(W,b)}}{\\partial{b}} = -\\frac{1}{m}\\sum_{i=1}^{m}\\left(1\\{y^{(i)}\\} - p(y^{(i)}=j|x^{(i)};W,b)\\right)$$\n",
    "\n",
    "A more advanced option is the L-BFGS alogrithm, which also requires the gradient function as an input argument.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model with Theano\n",
    "\n",
    "#### Vectorising the model\n",
    "\n",
    "Notations\n",
    "* X: data matrix of size $n\\times m$, where $n$ is the number of dimensions and m the number of instances. That is, each column stores an instance.\n",
    "* W: weight matrix of size $k \\times n$, where $k$ is the number of classes.\n",
    "* b: bias vector of size $k\\times 1$.\n",
    "\n",
    "The posterior probability is\n",
    "\\begin{equation}\n",
    "p = softmax(WX+b)\n",
    "\\end{equation}\n",
    "where softmax(M) = M / M.sum(axis=0) and M is a arbitrary matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities for manipulating the paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_size(shape):\n",
    "    \"\"\"\n",
    "    count the number of elements in a ndarray with shape=shape\n",
    "    \"\"\"\n",
    "    size = 1\n",
    "    for i in shape:\n",
    "        size *= i\n",
    "    return size\n",
    "    \n",
    "def pack(param_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        param_list: list of ndarrays\n",
    "    Returns:\n",
    "        theta:  vector of shape (?,), flattened params\n",
    "        shapes: list of tuples, with each tuple storing the shape of each ndarray in param_list\n",
    "    \"\"\"\n",
    "    shapes = []\n",
    "    theta=None\n",
    "    for p in param_list:\n",
    "        size = p.size\n",
    "        p2 = p.reshape((size, )) \n",
    "        if theta is None:\n",
    "            theta = p2\n",
    "        else:\n",
    "            theta = np.hstack((theta, p2))\n",
    "        shapes += [p.shape] \n",
    "    return theta, shapes\n",
    "        \n",
    "def unpack(theta, shapes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        theta:  vector of shape (?,), flattened params\n",
    "        shapes: list of tuples, with each tuple storing the shape of each ndarray in param_list      \n",
    "    Returns:\n",
    "        param_list: list of ndarrays\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    params = []\n",
    "    for shape in shapes:\n",
    "        size = get_size(shape)\n",
    "        x = theta[i:i+size]\n",
    "        params += [x.reshape(shape)]\n",
    "        i += size\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the softmax regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import gzip\n",
    "import cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from param_util import pack, unpack\n",
    "\n",
    "# for debugging\n",
    "theano.config.optimizer=\"fast_run\"\n",
    "theano.config.exception_verbosity=\"high\"\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "    def __init__(self, n_in, n_out, L2_reg_coef, max_iter=100):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.L2_reg_coef = L2_reg_coef\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        self.W = theano.shared(\n",
    "            value=0.005 * np.random.randn(self.n_out, self.n_in),\n",
    "            name='W',\n",
    "            borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            value=np.zeros((self.n_out,), dtype=theano.config.floatX),\n",
    "            name='b',\n",
    "            borrow=True)\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "        # sample matrix, each column stores a sample\n",
    "        X = T.dmatrix('X')\n",
    "        # label\n",
    "        y = T.lvector('y')\n",
    "        # predict\n",
    "        linear_input = (T.dot(self.W, X).transpose() + self.b).transpose()\n",
    "        p_y_given_x = self.__softmax__(linear_input)\n",
    "        pred = T.argmax(p_y_given_x, axis=0)\n",
    "        # NLL: negative log-likelihood\n",
    "        nll = -T.mean(T.log(p_y_given_x[y, T.arange(0, y.shape[0])]))\n",
    "        # cost (loss)\n",
    "        cost = nll + self.L2_reg_coef * (self.W**2).sum()\n",
    "        # error rate\n",
    "        error = 1.0*T.sum(T.neq(pred, y))/X.shape[1]\n",
    "        \n",
    "        self.test_model = theano.function(\n",
    "            inputs=[X, y],\n",
    "            outputs=error)\n",
    "        self.run_model = theano.function(\n",
    "            inputs=[X],\n",
    "            outputs=pred)\n",
    "            \n",
    "        # compute gradient\n",
    "        grads = T.grad(cost, self.params)\n",
    "        params = [p.type() for p in self.params]\n",
    "        self.grad_func = theano.function(\n",
    "            inputs=params + [X, y],\n",
    "            outputs=grads,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "        self.cost_func = theano.function(\n",
    "            inputs=params + [X, y],\n",
    "            outputs=cost,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])   \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        init_theta, shapes = pack([self.W.get_value(), self.b.get_value()])\n",
    "        opt_theta, opt_cost, d = sp.optimize.fmin_l_bfgs_b(\n",
    "            func=self.__cost_and_grad__,\n",
    "            x0=init_theta,\n",
    "            fprime=None,\n",
    "            args=(shapes, X, y),\n",
    "            maxiter=self.max_iter,\n",
    "            iprint=1)\n",
    "        opt_param_list = unpack(opt_theta, shapes)\n",
    "        for shared_param, opt_param in zip(self.params, opt_param_list):\n",
    "             shared_param.set_value(opt_param)      \n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.run_model(X)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        return self.test_model(X, y)\n",
    "        \n",
    "    def __cost_and_grad__(self, theta, *args):\n",
    "        shapes = args[0]\n",
    "        X = args[1]\n",
    "        y = args[2]\n",
    "        param_list = unpack(theta, shapes)\n",
    "        cost =  self.cost_func(*(param_list + [X, y]))\n",
    "        grad = self.grad_func(*(param_list + [X, y]))\n",
    "        grad, _ = pack(grad)\n",
    "        return cost, grad        \n",
    "    \n",
    "    def __softmax__(self, M):\n",
    "        \"\"\"\n",
    "        normalise along the vertical axis\n",
    "        \"\"\"\n",
    "        e_M = T.exp(M - M.max(axis=0, keepdims=True))\n",
    "        return e_M / e_M.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model\n",
    "\n",
    "#### Dataset: MNIST Dataset of Handwritten Digits\n",
    "\n",
    "* Gray-scale image of size $28\\times 28$ with value ranging from [0, 1].\n",
    "* 50,000 training samples, 10,000 validation samples and 10,000 testing samples.\n",
    "\n",
    "![image](mnist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 training samples of dim 784\n",
      "error rate on test set is 8.5%, accuracy is 91.5%\n",
      "baseline: error rate on test set is 90.18%, accurary is 9.82%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    plt.close('all')\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    f = gzip.open('data/mnist.pkl.gz')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    \n",
    "    # train model\n",
    "    train_X = train_set[0].transpose()\n",
    "    train_y = train_set[1]\n",
    "    valid_X = valid_set[0].transpose()\n",
    "    valid_y = valid_set[1]\n",
    "    \n",
    "    train_X = np.hstack((train_X, valid_X))\n",
    "    train_y = np.hstack((train_y, valid_y))\n",
    "    \n",
    "    train_X = train_X[:, 0:60001:5]\n",
    "    train_y = train_y[0:60001:5]\n",
    "    \n",
    "    n_in = train_X.shape[0]\n",
    "    n_out = np.unique(train_y).shape[0]\n",
    "    print \"{n} training samples of dim {d}\".format(n=train_X.shape[1], d=n_in)\n",
    "    \n",
    "    softmax_regression = SoftmaxRegression(n_in, n_out, L2_reg_coef=1e-4, max_iter=100)\n",
    "    softmax_regression.fit(train_X, train_y)\n",
    "    \n",
    "    # test model\n",
    "    test_X = test_set[0].transpose()\n",
    "    test_y = test_set[1]\n",
    "    error = softmax_regression.evaluate(test_X, test_y)\n",
    "    print 'error rate on test set is {e}%, accuracy is {a}%'.format(e=error*100, a=100-100*error)\n",
    "    # baseline method\n",
    "    pred_baseline = np.random.randint(\n",
    "        low = np.min(train_y),\n",
    "        high = np.max(train_y)+1,\n",
    "        size=valid_y.shape)\n",
    "    error_baseline = 1.0 * (pred_baseline != valid_y).sum() / valid_X.shape[1]\n",
    "    print 'baseline: error rate on test set is {e}%, accurary is {a}%'.format(e=error_baseline*100, a=100-100*error_baseline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the \"standard digits\" in the eyes of Softmax Regression?\n",
    "\n",
    "Find input $x$ that maximally activates the $k$-th output unit and has limited $L_2$ norm: \n",
    "$$\n",
    "\\max_{x}h_k(x)\\\\\n",
    "s.t. \\Vert x \\Vert^2 \\le 1\n",
    "$$\n",
    "This problem can be approximated by a non-constrained minimisation problem:\n",
    "$$\n",
    "\\min_{x}\\left[-h_k(x) + \\lambda \\max(\\Vert x \\Vert^2 - 1, 0)\\right]\n",
    "$$\n",
    "where $\\lambda$ is a fairly large number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||x||^2 is 1.00, proab is 0.9381\n",
      "||x||^2 is 1.00, proab is 0.9924\n",
      "||x||^2 is 1.00, proab is 0.9972\n",
      "||x||^2 is 1.00, proab is 0.9806\n",
      "||x||^2 is 1.00, proab is 0.9873\n",
      "||x||^2 is 1.00, proab is 0.9997\n",
      "||x||^2 is 1.00, proab is 0.9800\n",
      "||x||^2 is 1.00, proab is 0.9986\n",
      "||x||^2 is 1.01, proab is 0.6853\n",
      "||x||^2 is 1.00, proab is 0.9644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taiqing/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/matplotlib/image.py:649: UserWarning: The backend (<class 'matplotlib.backends.backend_macosx.RendererMac'>) does not support interpolation='none'. The image will be interpolated with 'nearest` mode.\n",
      "  \"mode.\" % renderer.__class__)\n"
     ]
    }
   ],
   "source": [
    "# visulisation\n",
    "plt.close('all')\n",
    "def rescale(x):\n",
    "    # Remove DC (mean of images)\n",
    "    x = x - x.mean()\n",
    "    # Truncate to +/-3 standard deviations and scale to -1 to 1\n",
    "    pstd = 3 * x.std()\n",
    "    x = np.maximum(np.minimum(x, pstd), -pstd) / pstd\n",
    "    # Rescale from [-1,1] to [0.1,0.9]\n",
    "    x = (x + 1) * 0.4 + 0.1;\n",
    "    return x \n",
    "def softmax(M):\n",
    "    e_M = T.exp(M - M.max(axis=0, keepdims=True))\n",
    "    return e_M / e_M.sum(axis=0, keepdims=True)\n",
    "W = softmax_regression.W.get_value()\n",
    "b = softmax_regression.b.get_value()\n",
    "fig = plt.figure(0)\n",
    "for i in np.arange(0, 10):\n",
    "    x = T.dvector('x')\n",
    "    x0 = np.random.rand(28*28)\n",
    "    x0 = x0 / np.sqrt((x0**2).sum())\n",
    "    p_y_given_x = softmax(T.dot(W, x) + b)\n",
    "    # cost\n",
    "    cost = -T.log(p_y_given_x[i])  + 1e2 * T.max([0, (x**2).sum()-1])**2\n",
    "    cost_func = theano.function([x], cost)\n",
    "    grad = T.grad(cost, x)\n",
    "    grad_func = theano.function([x], grad)\n",
    "    proba_func = theano.function([x], p_y_given_x)\n",
    "    def cost_and_grad(x):\n",
    "        return cost_func(x), grad_func(x)\n",
    "    opt_x, opt_cost, d = sp.optimize.fmin_l_bfgs_b(\n",
    "        func=cost_and_grad,\n",
    "        x0=x0,\n",
    "        maxiter=100,\n",
    "        iprint=0)\n",
    "    #print opt_x\n",
    "    print \"||x||^2 is {v:.2f}, proab is {p:.4f}\".format(v=(opt_x**2).sum(), p=proba_func(opt_x)[i])\n",
    "    opt_x_rescaled = rescale(opt_x)\n",
    "    img = opt_x_rescaled.reshape((28,28))\n",
    "    ax = fig.add_subplot(2, 5,i+1)\n",
    "    ax.imshow(img, cmap='gray', interpolation='none')\n",
    "    ax.set_axis_off()\n",
    "plt.show()\n",
    "fig.savefig('visual_softmax_regression.png', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](visual_softmax_regression_digits.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highlights of DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "#### Neuron: Building block of neural network\n",
    "\n",
    "![image](neuron_brain.jpg)\n",
    "![image](neuron.png)\n",
    "\n",
    "$$h_{W,b}(x) = f(W^Tx+b) = f(\\sum_{i=1}^{3}W_ix_i + b)$$\n",
    ", where $f: \\mathcal{R}\\rightarrow\\mathcal{R}$ is called the activation function. Common choices for $f$ include the sigmoid function and the tanh function. We use the sigmoid function in this talk.\n",
    "\n",
    "sigmoid function:\n",
    "$$f(z) = \\frac{1}{1 + \\exp(-z)}$$\n",
    "\n",
    "tanh function:\n",
    "$$f(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$\n",
    "\n",
    "![image](sigmoid.png) ![image](tanh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network model\n",
    "\n",
    "![image](nn.png)\n",
    "\n",
    "##### Notations\n",
    "* $m$: the number of samples\n",
    "* $n_l$: the number of layers, including the input, hidden and output layers\n",
    "* $s_l$: the number of units in layer $L_l$, excluding the bias unit\n",
    "* $W^{(l)}$: the weight matrix connecting layer $L_l$ and layer $L_{l+1}$\n",
    "* $W_{ji}^{(l)}$: the weight connecting unit $i$ in layer $L_l$ and unit $j$ in layer $L_{l+1}$\n",
    "* $b^{(l)}$: the bias vector connecting the bias unit in layer $L_l$ to the units in layer $L_{l+1}$\n",
    "* $z^{(l)}_j$: the linear input of the unit $j$ in layer $L_l$\n",
    "* $a^{(l)}_j$: the activation of the unit $j$ in layer $L_l$\n",
    "\n",
    "![image](simple_nn.png)\n",
    "\n",
    "$$a_1^{(2)} = f(W_{11}^{(1)}x_1 + W_{12}^{(1)}x_1 + W_{13}^{(1)}x_1 + b_1^{(1)})$$\n",
    "\n",
    "$$a_2^{(2)} = f(W_{21}^{(1)}x_1 + W_{22}^{(1)}x_1 + W_{23}^{(1)}x_1 + b_1^{(2)})$$\n",
    "\n",
    "$$a_1^{(3)} = f(W_{31}^{(1)}x_1 + W_{32}^{(1)}x_1 + W_{33}^{(1)}x_1 + b_1^{(3)})$$\n",
    "\n",
    "$$h_{W,b}(x) = a_1^{(3)} = f(W_{11}^{(2)}a_1 + W_{12}^{(2)}a_1 + W_{13}^{(2)}a_1 + b_1^{(2)})$$\n",
    "\n",
    "If we extend the activation function $f(\\cdot)$ to apply to vectors in an element-wise fashion (i.e., $f([z_1, z_2, z_3]) = [f(z_1), f(z_2), f(z_3)])$, we can rewrite the equations above more compactly as\n",
    "\n",
    "$$z^{(2)} = W^{(1)}x + b^{(1)}$$\n",
    "\n",
    "$$a^{(2)} = f(z^{(2)})$$\n",
    "\n",
    "$$z^{(3)} = W^{(2)}a^{(2)} + b^{(2)}$$\n",
    "\n",
    "$$h_{W,b}(x) = a^{(3)} = f(z^{(3)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation - RUNNING\n",
    "\n",
    "Set $a^{(1)} = x$ to denote the values from the input layer. Given layer $L_l$'s activations $a^{(l)}$, we can compute layer $L_{l+1}$'s activations $a^{(l+1)}$ as\n",
    "\n",
    "$$z^{(l+1)} = W^{(l)}a^{(l)} + b^{(l)}$$\n",
    "\n",
    "$$a^{(l+1)} = f(z^{(l+1)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation - TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "Suppose we have only a set of unlabeled training examples $\\{x^{(1)},...,x^{(m)}\\}$, where $x^{(i)}\\in R^n$.\n",
    "An autoencoder neural network is an unsupervised learning algorithm that applies backpropagation, setting the target values to be equal to the inputs, i.e., $y^{(i)}=x^{(i)}$.\n",
    "\n",
    "![image](autoencoder.png)\n",
    "\n",
    "The autoencoder tries to learn an identity function $h_{W,b}\\approx x$.\n",
    "* The identity function seems a particularly trivial function to be trying to learn;\n",
    "* By placing constraints on the network, such as by limiting the number of hidden units, we can discover interesting structure about the data.\n",
    "* As a concrete example, suppose the inputs $x$ are the pixel intensity values from a $10\\times 10$ image ($100$ pixels) so $n=100$, and there are $s_2=50$ hidden units in layer $L_2$. \n",
    "Since there are only $50$ hidden units, the network is forced to learn a compressed representation of the input.\n",
    "* If the inputs are completely random, i.e., each dimension comes from an independent distribution, the autoencoding task would be very difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notations\n",
    "* $m$: the number of samples\n",
    "* $n_l$: the number of layers, including the input, hidden and output layers\n",
    "* $s_l$: the number of units in layer $L_l$, excluding the bias unit\n",
    "* $W^{(l)}$: the weight matrix connecting layer $L_l$ and layer $L_{l+1}$\n",
    "* $W_{ji}^{(l)}$: the weight connecting unit $i$ in layer $L_l$ and unit $j$ in layer $L_{l+1}$\n",
    "* $b^{(l)}$: the bias vector connecting the bias unit in layer $L_l$ to the units in layer $L_{l+1}$\n",
    "* $z^{(l)}_j$: the linear input of the unit $j$ in layer $L_l$\n",
    "* $a^{(l)}_j$: the activation of the unit $j$ in layer $L_l$\n",
    "\n",
    "Foward propagation\n",
    "$$z^{(2)} = W^{(1)}x + b^{(1)}$$\n",
    "\n",
    "$$a^{(2)} = \\text{sigmoid}(z^{(2)})$$\n",
    "\n",
    "$$z^{(3)} = W^{(2)}a^{(2)} + b^{(2)}$$\n",
    "\n",
    "$$a^{(3)} = \\text{sigmoid}(z^{(3)})$$\n",
    "\n",
    "$$h_{W,b}(x) = a^{(3)}(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Loss Function\n",
    "\n",
    "For a single training sample $x$, the loss function is defined as\n",
    "\\begin{equation}\n",
    "J(W,b; x) = \\frac{1}{2}\\Vert h_{W,b}(x)-x\\Vert^2\n",
    "\\end{equation}\n",
    "Given a trainig set of $m$ samoples, we define the loss function as\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "J(W,b) &= \n",
    "\\left[\\frac{1}{m}\\sum_{i=1}^{m}J(W,b; x^{(i)})\\right]\n",
    "+\\lambda \\sum_{l=1}^{n_{l-1}}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}\\left(W_{ji}^{(l)}\\right)^2\\\\\n",
    "&=\\left[\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\frac{1}{2}\\Vert h_{W,}(x^{(i)})-x^{(i)}\\Vert^2\\right)\\right]\n",
    "+\\lambda \\sum_{l=1}^{n_{l-1}}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}\\left(W_{ji}^{(l)}\\right)^2\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The second term is a weight decay term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity Constraint\n",
    "\n",
    "We would like to constrain the neurons to be active only for a subset of patterns.\n",
    "* $a^{(2)}_j(x)$ denotes the activiation of hidden unit $j$ when the network is given a specific input $x$.\n",
    "* The average activation of hidden unit $j$ over the training set is\n",
    "\\begin{equation}\n",
    "\\hat{\\rho}_j = \\frac{1}{m}\\sum_{i=1}^{m}\\left[a^{(2)}_j(x)\\right]\n",
    "\\end{equation}\n",
    "* We would like to enforce the constraint\n",
    "$$\\hat{\\rho}_j=\\rho$$\n",
    "where $\\rho$ is a sparsity parameter, typically a small value close to zero.\n",
    "* A penalty term that penalises $\\hat{\\rho}_j$ deviating significantly from $\\rho$\n",
    "\\begin{equation}\n",
    "\\sum_{j=1}^{s_2}KL(\\rho||\\hat{\\rho}_j) = \\sum_{j=1}^{s_2}\\left[\\rho\\log\\frac{\\rho}{\\hat{\\rho}_j} + (1-\\rho)\\log\\frac{1-\\rho}{1-\\hat{\\rho}_j}\\right]\n",
    "\\end{equation}\n",
    "    * $KL(\\rho||\\hat{\\rho}_j)$ has the property that $KL(\\rho||\\hat{\\rho}_j) = 0$ if $\\hat{\\rho}_j=\\rho$, and otherwise it increases monotonically as $\\hat{\\rho}_j$ diverges from $\\rho$.\n",
    "    * In the figure below, we set $\\rho = 0.2$.\n",
    "\n",
    "![image](kl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our overall loss function for sparse autoencoder is\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "J(W,b) &= \n",
    "\\left[\\frac{1}{m}\\sum_{i=1}^{m}J(W,b; x^{(i)})\\right]\n",
    "+\\lambda \\sum_{l=1}^{n_{l-1}}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}\\left(W_{ji}^{(l)}\\right)^2 + \\beta\\sum_{j=1}^{s_2}KL(\\rho||\\hat{\\rho}_j)\\\\\n",
    "&=\\left[\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\frac{1}{2}\\Vert h_{W,b}(x^{(i)})-x^{(i)}\\Vert^2\\right)\\right]\n",
    "+\\lambda \\sum_{l=1}^{n_{l-1}}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}\\left(W_{ji}^{(l)}\\right)^2 + \\beta \\sum_{j=1}^{s_2}\\left[\\rho\\log\\frac{\\rho}{\\hat{\\rho}_j} + (1-\\rho)\\log\\frac{1-\\rho}{1-\\hat{\\rho}_j}\\right]\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model with Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities for paramter manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import numpy as np\n",
    "\n",
    "def get_size(shape):\n",
    "    \"\"\"\n",
    "    count the number of elements in a ndarray with shape=shape\n",
    "    \"\"\"\n",
    "    size = 1\n",
    "    for i in shape:\n",
    "        size *= i\n",
    "    return size\n",
    "    \n",
    "def pack(param_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        param_list: list of ndarrays\n",
    "    Returns:\n",
    "        theta:  vector of shape (?,), flattened params\n",
    "        shapes: list of tuples, with each tuple storing the shape of each ndarray in param_list\n",
    "    \"\"\"\n",
    "    shapes = []\n",
    "    theta=None\n",
    "    for p in param_list:\n",
    "        size = p.size\n",
    "        p2 = p.reshape((size, )) \n",
    "        if theta is None:\n",
    "            theta = p2\n",
    "        else:\n",
    "            theta = np.hstack((theta, p2))\n",
    "        shapes += [p.shape] \n",
    "    return theta, shapes\n",
    "        \n",
    "def unpack(theta, shapes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        theta:  vector of shape (?,), flattened params\n",
    "        shapes: list of tuples, with each tuple storing the shape of each ndarray in param_list      \n",
    "    Returns:\n",
    "        param_list: list of ndarrays\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    params = []\n",
    "    for shape in shapes:\n",
    "        size = get_size(shape)\n",
    "        x = theta[i:i+size]\n",
    "        params += [x.reshape(shape)]\n",
    "        i += size\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, n_in, n_out, name=''):\n",
    "        r = -np.sqrt(6.0 / (n_in + n_out + 1))\n",
    "        rand_W = 2 * r * np.random.rand(n_out, n_in) - r\n",
    "        self.W = theano.shared(\n",
    "            value=rand_W,\n",
    "            name='W_'+name,\n",
    "            borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            value=np.zeros((n_out,), dtype=theano.config.floatX),\n",
    "            name='b_'+name,\n",
    "            borrow=True)\n",
    "        self.params = [self.W, self.b]\n",
    "        self.L2_sqr = (self.W ** 2).sum()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: matrix tensor, each column stores a sample\n",
    "        \"\"\"\n",
    "        linear_input = (T.dot(self.W, X).transpose() + self.b).transpose()\n",
    "        activation = T.nnet.sigmoid(linear_input)\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a AutoEncoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import gzip\n",
    "import cPickle\n",
    "\n",
    "#from param_util import pack, unpack\n",
    "#from layer import Layer\n",
    "\n",
    "# for debugging\n",
    "theano.config.optimizer='fast_run'#'fast_compile'\n",
    "theano.config.exception_verbosity=\"high\"\n",
    "\n",
    "class AutoEncoder(object):\n",
    "    def __init__(self, n_in, n_hid, \n",
    "                 L2_reg_coef, sparse_reg_coef, sparse_rho, \n",
    "                 max_iter=400):\n",
    "        self.n_in = n_in\n",
    "        self.n_hid = n_hid\n",
    "        self.L2_reg_coef = L2_reg_coef\n",
    "        self.sparse_reg_coef = sparse_reg_coef\n",
    "        self.sparse_rho = sparse_rho\n",
    "        self.max_iter = max_iter \n",
    "        \n",
    "        self.hidden_layer = Layer(self.n_in, self.n_hid, 'hidden')\n",
    "        self.output_layer = Layer(self.n_hid, self.n_in, 'output')\n",
    "        self.params = self.hidden_layer.params + self.output_layer.params\n",
    "        \n",
    "        X = T.dmatrix(name='X') # each column stores a sample\n",
    "        m = X.shape[1] # sample count\n",
    "        activation = self.hidden_layer.forward(X)\n",
    "        output = self.output_layer.forward(activation)\n",
    "        error = ((output - X) ** 2).sum() / (2.0 * m)\n",
    "        L2_reg = self.hidden_layer.L2_sqr + self.output_layer.L2_sqr\n",
    "        rho = activation.mean(axis=1)\n",
    "        kl = (self.sparse_rho * T.log(self.sparse_rho/rho) + \n",
    "              (1-self.sparse_rho) * T.log((1-self.sparse_rho)/(1-rho))).sum()\n",
    "        cost = error + self.L2_reg_coef * 0.5 * L2_reg + self.sparse_reg_coef * kl\n",
    "        grads = T.grad(cost, self.params)\n",
    "        params = [p.type() for p in self.params]\n",
    "        self.cost_func = theano.function(\n",
    "            inputs=params + [X],\n",
    "            outputs=cost,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "        self.grad_func = theano.function(\n",
    "            inputs=params + [X],\n",
    "            outputs=grads,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "        self.run_model = theano.function(\n",
    "            inputs=[X],\n",
    "            outputs=activation)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: matrix tensor, each column stores a sample\n",
    "        \"\"\"\n",
    "        return self.hidden_layer.forward(X)\n",
    "            \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        X: np.ndarray, each column stores a sample\n",
    "        \"\"\"\n",
    "        init_theta, shapes = pack([\n",
    "            self.hidden_layer.W.get_value(), self.hidden_layer.b.get_value(), \n",
    "            self.output_layer.W.get_value(), self.output_layer.b.get_value()])\n",
    "        opt_theta, opt_cost, d = sp.optimize.fmin_l_bfgs_b(\n",
    "            func=self.cost_and_grad,\n",
    "            x0=init_theta,\n",
    "            fprime=None,\n",
    "            args=(shapes, X),\n",
    "            maxiter=self.max_iter,\n",
    "            iprint=1)\n",
    "        opt_param_list = unpack(opt_theta, shapes)\n",
    "        for shared_param, opt_param in zip(self.params, opt_param_list):\n",
    "             shared_param.set_value(opt_param)\n",
    "             \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        X: np.ndarray, each column stores a sample\n",
    "        \"\"\"\n",
    "        return self.run_model(X)\n",
    "\n",
    "    def cost_and_grad(self, theta, *args):\n",
    "        \"\"\"\n",
    "        compute the cost and gradient at theta in the parameter space\n",
    "        args should be (shapes, X)\n",
    "        X: each column stores a sample\n",
    "        \"\"\"\n",
    "        shapes = args[0]\n",
    "        X = args[1]\n",
    "        param_list = unpack(theta, shapes)\n",
    "        cost =  self.cost_func(*(param_list+[X]))\n",
    "        grad = self.grad_func(*(param_list+[X]))\n",
    "        grad, _ = pack(grad)\n",
    "        return cost, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 samples of dimension 784\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    plt.close('all')\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    #X = cPickle.load(open('data/patches', 'r'))\n",
    "    f = gzip.open('data/mnist.pkl.gz')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    train_X = train_set[0].transpose()\n",
    "    X = train_X[:, 0:50001:5]\n",
    "    print '{m} samples of dimension {d}'.format(m=X.shape[1], d=X.shape[0])\n",
    "    auto_encoder = AutoEncoder(\n",
    "        n_in=X.shape[0],\n",
    "        n_hid = 200,\n",
    "        L2_reg_coef=3e-3,\n",
    "        sparse_reg_coef=3.0,\n",
    "        sparse_rho=0.1,\n",
    "        max_iter=200)\n",
    "    auto_encoder.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising a Trained Autoencoder\n",
    "\n",
    "Given a specific input $x$, the activation of hidden unit $i$ is\n",
    "$$a^{(2)}_i(x) = \\text{sigmoid}(\\sum_{j=1}^{s_1}W_{ij}^{(1)}x_j + b_i)$$\n",
    "\n",
    "Find the input $x^*$ that maximises $a^{(2)}_i$, i.e., causes hidden unit $i$ to be maximally activated.\n",
    "\n",
    "Considering\n",
    "* Sigmoid function is a monotonically increasing function;\n",
    "* The input should be constrained to have a limited magnitude,\n",
    "\n",
    "we have\n",
    "\\begin{equation}\n",
    "x^* = \\arg\\max_{x}{\\sum_{j=1}^{s_1}W_{ij}^{(1)}x_j + b_i}\\\\\n",
    "s.t. \\Vert x \\Vert^2 \\le 1\n",
    "\\end{equation}\n",
    "\n",
    "** Lemma 1** The optimal $x^*$ has identity norm, that is\n",
    "$$\\Vert x^* \\Vert^2 = 1$$\n",
    "\n",
    "Solve for $x^*$ by lagrange multiplier:\n",
    "\\begin{equation}\n",
    "F(x, \\lambda) = {\\sum_{j=1}^{s_1}W_{ij}^{(1)}x_j + b_i} + \\lambda \\left(\\sum_{j=1}^{s_1}x_j^2-1\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Setting\n",
    "\\begin{equation}\n",
    "\\frac{\\partial F(x, \\lambda)}{\\partial x_j} = W_{ij}^{(1)}x_j + 2\\lambda x_j = 0\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial F(x, \\lambda)}{\\partial \\lambda} = \\sum_{j=1}^{s_1}x_j^2 - 1 = 0\n",
    "\\end{equation}\n",
    "\n",
    "By solving the above equations, we have\n",
    "\\begin{equation}\n",
    "x_j^* = \\frac{W_{ij}^{(1)}}{\\sqrt{\\sum_{j=1}^{s_1}\\left(W_{ij}^{(1)}\\right)^2}}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    fig = plt.figure(0)\n",
    "    n = 8#int(np.floor(np.sqrt(auto_encoder.n_hid)))\n",
    "    for i in np.arange(n**2):\n",
    "        ax = fig.add_subplot(n, n, i+1)\n",
    "        w = auto_encoder.hidden_layer.W.get_value()[i, :]\n",
    "        w = w / np.sqrt((w ** 2).sum())\n",
    "        w = (w - w.min()) / (w.max()-w.min())\n",
    "        d = np.floor(np.sqrt(w.shape[0]))\n",
    "        ax.imshow(w.reshape((d,d)), cmap='gray', interpolation='none')\n",
    "        ax.set_axis_off()\n",
    "    plt.show()\n",
    "    fig.savefig('visual_ae.png', format='png')\n",
    "    \n",
    "    # visulisation the first hidden layer\n",
    "    def rescale(x):\n",
    "        # Remove DC (mean of images)\n",
    "        x = x - x.mean()\n",
    "        # Truncate to +/-3 standard deviations and scale to -1 to 1\n",
    "        pstd = 3 * x.std()\n",
    "        x = np.maximum(np.minimum(x, pstd), -pstd) / pstd\n",
    "        # Rescale from [-1,1] to [0.1,0.9]\n",
    "        x = (x + 1) * 0.4 + 0.1;\n",
    "        return x\n",
    "    fig = plt.figure(0)\n",
    "    k = 0\n",
    "    for i in range(0, 25):#for i in np.random.randint(0, dnn.ae_layers[0].n_hid, 64):\n",
    "        x = T.dvector('x')\n",
    "        x0 = np.random.rand(28*28)\n",
    "        x0 = x0 / np.sqrt((x0**2).sum())\n",
    "        a = auto_encoder.forward(x)\n",
    "        # cost\n",
    "        cost = -a[i] + 1e2 * T.max([0, (x**2).sum()-1])**2\n",
    "        grad = T.grad(cost, x)\n",
    "        cost_func = theano.function([x], cost)\n",
    "        grad = T.grad(cost, x)\n",
    "        grad_func = theano.function([x], grad)\n",
    "        act_func = theano.function([x], a)\n",
    "        def cost_and_grad(x):\n",
    "            return cost_func(x), grad_func(x)\n",
    "        opt_x, opt_cost, d = sp.optimize.fmin_l_bfgs_b(\n",
    "            func=cost_and_grad,\n",
    "            x0=x0,\n",
    "            maxiter=1000,\n",
    "            iprint=0)\n",
    "        opt_x_rescaled = rescale(opt_x)\n",
    "        img = opt_x_rescaled.reshape((28,28))\n",
    "        k = k + 1\n",
    "        ax = fig.add_subplot(5, 5, k)\n",
    "        ax.imshow(img, cmap='gray', interpolation='none')\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "        w = auto_encoder.hidden_layer.W.get_value()[i,:]\n",
    "        w = w / np.sqrt((w ** 2).sum())\n",
    "        print (\"||x||^2 is {v:.2f}, activation is {a:.4f}, {t:.4f} (optimal)\"\n",
    "            .format(v=(opt_x**2).sum(), a=act_func(opt_x)[i], t=act_func(w)[i]))\n",
    "    plt.show()\n",
    "    fig.savefig('visual_ae2.png', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](visual_autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Deep Networks for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Advantages of Deep Networks\n",
    "* Deep network can compactly represent a significantly larger set of functions than shallow networks.\n",
    "    * For a network with $n$ inputs, $l$ hidden layers and sigmoid activation, the bound of the complexity of the function implemented by a feedforward neural network is $2^{l-1}$. [M. Bianchini and F. Scarselli, On the complexity of shallow and deep neural network classifiers, ESANN14]\n",
    "* By using a deep network, one can start to learn part-whole decompositions in the case of images.\n",
    "    * The first layer might learn to detect edges,\n",
    "    * The second layer might learn to group edges to detect longer contours,\n",
    "    * The thirt layer might learn to detect simple parts of objects, and so on.\n",
    "    ![image](part_whole.png)\n",
    "* Cortical computations in the brain also have multiple layers of processing.\n",
    "    * Visual images are processed in multiple stages by the brain, by cortical area \"V1\", followed by cortical area \"V2\", and so on.\n",
    "    ![image](cortical.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difficulties of Training Deep Architectures\n",
    "\n",
    "The main learning algorithmthat researchers were using was to randomly initialise the weights of the a deep network, and then train it using a labeled training set $\\{(x^{(i)}, y^{(i)}),i=1...m\\}$ using a supervised learning objective. However, this usually did not work well for deep networks.\n",
    "\n",
    "#### Availability of data\n",
    "* Given the high degree of expressive power of deep networks, training on insufficient data would result in overfitting.\n",
    "* Labeled data is often scarce and sometimes expensive.\n",
    "* For many problems, it is difficult to get enough samples to fit the parameters of a complex model.\n",
    "\n",
    "#### Local optima\n",
    "* Training a deep network using supervised learning involves solving a highly non-convex optimisation problem, which is rife with bad local optima.\n",
    "* Trainig with gradient descent (or methods like conjugate gradient and L-BFGS) can easily get trapped in bad local optima.\n",
    "\n",
    "#### Diffusion of gradients\n",
    "* The gradients that are propagated backwards rapidly diminish in magnitude as the depth of the network increases.\n",
    "* When using gradient descent, the weights of the earlier layers change slowly and the earlier layers fail to learn much.\n",
    "* Training a deep network ends up giving similar performance to training a shallow network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Layer-wise Training\n",
    "The main idea is \n",
    "* first to train the hidden layers of the network one at a time in an unsurpervised manner,\n",
    "* then to fine-tune the whole network with labeled data in a supervised manner.\n",
    "\n",
    "Advantages of greedy layer-wise training include\n",
    "#### Availabilty of data\n",
    "* Enable to take advantage of the cheap and plentiful unlabeled data\n",
    "\n",
    "#### Better local optima\n",
    "* The weights are now starting at a better location in parameter space than if they had been randomly initialised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model with Theano\n",
    "\n",
    "This section will implement a deep architecture with stacked autoencoders and a softmax regression as the output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities for manipluating parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import numpy as np\n",
    "\n",
    "def get_size(shape):\n",
    "    \"\"\"\n",
    "    count the number of elements in a ndarray with shape=shape\n",
    "    \"\"\"\n",
    "    size = 1\n",
    "    for i in shape:\n",
    "        size *= i\n",
    "    return size\n",
    "    \n",
    "def pack(param_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        param_list: list of ndarrays\n",
    "    Returns:\n",
    "        theta:  vector of shape (?,), flattened params\n",
    "        shapes: list of tuples, with each tuple storing the shape of each ndarray in param_list\n",
    "    \"\"\"\n",
    "    shapes = []\n",
    "    theta=None\n",
    "    for p in param_list:\n",
    "        size = p.size\n",
    "        p2 = p.reshape((size, )) \n",
    "        if theta is None:\n",
    "            theta = p2\n",
    "        else:\n",
    "            theta = np.hstack((theta, p2))\n",
    "        shapes += [p.shape] \n",
    "    return theta, shapes\n",
    "        \n",
    "def unpack(theta, shapes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        theta:  vector of shape (?,), flattened params\n",
    "        shapes: list of tuples, with each tuple storing the shape of each ndarray in param_list      \n",
    "    Returns:\n",
    "        param_list: list of ndarrays\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    params = []\n",
    "    for shape in shapes:\n",
    "        size = get_size(shape)\n",
    "        x = theta[i:i+size]\n",
    "        params += [x.reshape(shape)]\n",
    "        i += size\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, n_in, n_out, name=''):\n",
    "        r = -np.sqrt(6.0 / (n_in + n_out + 1))\n",
    "        rand_W = 2 * r * np.random.rand(n_out, n_in) - r\n",
    "        self.W = theano.shared(\n",
    "            value=rand_W,\n",
    "            name='W_'+name,\n",
    "            borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            value=np.zeros((n_out,), dtype=theano.config.floatX),\n",
    "            name='b_'+name,\n",
    "            borrow=True)\n",
    "        self.params = [self.W, self.b]\n",
    "        self.L2_sqr = (self.W ** 2).sum()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: matrix tensor, each column stores a sample\n",
    "        \"\"\"\n",
    "        linear_input = (T.dot(self.W, X).transpose() + self.b).transpose()\n",
    "        activation = T.nnet.sigmoid(linear_input)\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AutoEncoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import gzip\n",
    "import cPickle\n",
    "\n",
    "#from param_util import pack, unpack\n",
    "#from layer import Layer\n",
    "\n",
    "# for debugging\n",
    "theano.config.optimizer='fast_run'#'fast_compile'\n",
    "theano.config.exception_verbosity=\"high\"\n",
    "\n",
    "class AutoEncoder(object):\n",
    "    def __init__(self, n_in, n_hid, \n",
    "                 L2_reg_coef, sparse_reg_coef, sparse_rho, \n",
    "                 max_iter=400):\n",
    "        self.n_in = n_in\n",
    "        self.n_hid = n_hid\n",
    "        self.L2_reg_coef = L2_reg_coef\n",
    "        self.sparse_reg_coef = sparse_reg_coef\n",
    "        self.sparse_rho = sparse_rho\n",
    "        self.max_iter = max_iter \n",
    "        \n",
    "        self.hidden_layer = Layer(self.n_in, self.n_hid, 'hidden')\n",
    "        self.output_layer = Layer(self.n_hid, self.n_in, 'output')\n",
    "        self.params = self.hidden_layer.params + self.output_layer.params\n",
    "        \n",
    "        X = T.dmatrix(name='X') # each column stores a sample\n",
    "        m = X.shape[1] # sample count\n",
    "        activation = self.hidden_layer.forward(X)\n",
    "        output = self.output_layer.forward(activation)\n",
    "        error = ((output - X) ** 2).sum() / (2.0 * m)\n",
    "        L2_reg = self.hidden_layer.L2_sqr + self.output_layer.L2_sqr\n",
    "        rho = activation.mean(axis=1)\n",
    "        kl = (self.sparse_rho * T.log(self.sparse_rho/rho) + \n",
    "              (1-self.sparse_rho) * T.log((1-self.sparse_rho)/(1-rho))).sum()\n",
    "        cost = error + self.L2_reg_coef * 0.5 * L2_reg + self.sparse_reg_coef * kl\n",
    "        grads = T.grad(cost, self.params)\n",
    "        params = [p.type() for p in self.params]\n",
    "        self.cost_func = theano.function(\n",
    "            inputs=params + [X],\n",
    "            outputs=cost,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "        self.grad_func = theano.function(\n",
    "            inputs=params + [X],\n",
    "            outputs=grads,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "        self.run_model = theano.function(\n",
    "            inputs=[X],\n",
    "            outputs=activation)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: matrix tensor, each column stores a sample\n",
    "        \"\"\"\n",
    "        return self.hidden_layer.forward(X)\n",
    "            \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        X: np.ndarray, each column stores a sample\n",
    "        \"\"\"\n",
    "        init_theta, shapes = pack([\n",
    "            self.hidden_layer.W.get_value(), self.hidden_layer.b.get_value(), \n",
    "            self.output_layer.W.get_value(), self.output_layer.b.get_value()])\n",
    "        opt_theta, opt_cost, d = sp.optimize.fmin_l_bfgs_b(\n",
    "            func=self.cost_and_grad,\n",
    "            x0=init_theta,\n",
    "            fprime=None,\n",
    "            args=(shapes, X),\n",
    "            maxiter=self.max_iter,\n",
    "            iprint=1)\n",
    "        opt_param_list = unpack(opt_theta, shapes)\n",
    "        for shared_param, opt_param in zip(self.params, opt_param_list):\n",
    "             shared_param.set_value(opt_param)\n",
    "             \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        X: np.ndarray, each column stores a sample\n",
    "        \"\"\"\n",
    "        return self.run_model(X)\n",
    "\n",
    "    def cost_and_grad(self, theta, *args):\n",
    "        \"\"\"\n",
    "        compute the cost and gradient at theta in the parameter space\n",
    "        args should be (shapes, X)\n",
    "        X: each column stores a sample\n",
    "        \"\"\"\n",
    "        shapes = args[0]\n",
    "        X = args[1]\n",
    "        param_list = unpack(theta, shapes)\n",
    "        cost =  self.cost_func(*(param_list+[X]))\n",
    "        grad = self.grad_func(*(param_list+[X]))\n",
    "        grad, _ = pack(grad)\n",
    "        return cost, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SoftmaxRegression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import gzip\n",
    "import cPickle\n",
    "\n",
    "from param_util import pack, unpack\n",
    "\n",
    "# for debugging\n",
    "theano.config.optimizer=\"fast_run\"\n",
    "theano.config.exception_verbosity=\"high\"\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "    def __init__(self, n_in, n_out, L2_reg_coef, max_iter=100):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.L2_reg_coef = L2_reg_coef\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        self.W = theano.shared(\n",
    "            value=0.005 * np.random.randn(self.n_out, self.n_in),\n",
    "            name='W',\n",
    "            borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            value=np.zeros((self.n_out, ), dtype=theano.config.floatX),\n",
    "            name='b',\n",
    "            borrow=True)\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "        # sample matrix, each column stores a sample\n",
    "        X = T.dmatrix('X')\n",
    "        # label\n",
    "        y = T.lvector('y')\n",
    "        # predict\n",
    "        pred, p_y_given_x = self.predict(X)\n",
    "        # cost\n",
    "        cost = self.calc_cost(p_y_given_x, y)\n",
    "        # error rate\n",
    "        error = 1.0*T.sum(T.neq(pred, y))/X.shape[1]\n",
    "        \n",
    "        self.test_model = theano.function(\n",
    "            inputs=[X, y],\n",
    "            outputs=error)\n",
    "        self.run_model = theano.function(\n",
    "            inputs=[X],\n",
    "            outputs=pred)\n",
    "            \n",
    "        # compute gradient\n",
    "        grads = T.grad(cost, self.params)\n",
    "        params = [p.type() for p in self.params]\n",
    "        self.grad_func = theano.function(\n",
    "            inputs=params + [X, y],\n",
    "            outputs=grads,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "        self.cost_func = theano.function(\n",
    "            inputs=params + [X, y],\n",
    "            outputs=cost,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "    \n",
    "    def calc_cost(self, p_y_given_x, y):\n",
    "        \"\"\"\n",
    "        p_y_given_x: matrix tensor, output by self.predict\n",
    "        y: vector tensor, labels\n",
    "        \"\"\"\n",
    "        # NLL: negative log-likelihood\n",
    "        nll = -T.mean(T.log(p_y_given_x[y, T.arange(0, y.shape[0])]))\n",
    "        cost = nll + self.L2_reg_coef * (self.W**2).sum()\n",
    "        return cost\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: matrix tensor, each column stores a sample\n",
    "        \"\"\"\n",
    "        linear_input = (T.dot(self.W, X).transpose() + self.b).transpose()\n",
    "        p_y_given_x = self.softmax(linear_input)\n",
    "        pred = T.argmax(p_y_given_x, axis=0)\n",
    "        return pred, p_y_given_x        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        init_theta, shapes = pack([self.W.get_value(), self.b.get_value()])\n",
    "        opt_theta, opt_cost, d = sp.optimize.fmin_l_bfgs_b(\n",
    "            func=self.cost_and_grad,\n",
    "            x0=init_theta,\n",
    "            fprime=None,\n",
    "            args=(shapes, X, y),\n",
    "            maxiter=self.max_iter,\n",
    "            iprint=1)\n",
    "        opt_param_list = unpack(opt_theta, shapes)\n",
    "        for shared_param, opt_param in zip(self.params, opt_param_list):\n",
    "             shared_param.set_value(opt_param)      \n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.run_model(X)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        return self.test_model(X, y)\n",
    "        \n",
    "    def cost_and_grad(self, theta, *args):\n",
    "        shapes = args[0]\n",
    "        X = args[1]\n",
    "        y = args[2]\n",
    "        param_list = unpack(theta, shapes)\n",
    "        cost =  self.cost_func(*(param_list + [X, y]))\n",
    "        grad = self.grad_func(*(param_list + [X, y]))\n",
    "        grad, _ = pack(grad)\n",
    "        return cost, grad        \n",
    "    \n",
    "    def softmax(self, M):\n",
    "        \"\"\"\n",
    "        normalise along the vertical axis\n",
    "        \"\"\"\n",
    "        e_M = T.exp(M - M.max(axis=0, keepdims=True))\n",
    "        return e_M / e_M.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN class\n",
    "(TODO: fine-tuning equations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import gzip\n",
    "import cPickle\n",
    "\n",
    "#from param_util import pack, unpack\n",
    "#from autoencoder import AutoEncoder\n",
    "#from softmax_regression import SoftmaxRegression\n",
    "\n",
    "# for debugging\n",
    "theano.config.optimizer='fast_run'#'fast_compile'\n",
    "theano.config.exception_verbosity=\"high\"\n",
    "\n",
    "class DNN(object):\n",
    "    \"\"\"\n",
    "    stacked autoencoder with a softmax output layer\n",
    "    \"\"\"\n",
    "    def __init__(self, ae_layers, softmax_layer, finetune_max_iter=100, prefix='dnn'):\n",
    "        self.ae_layers = ae_layers\n",
    "        self.softmax_layer = softmax_layer\n",
    "        self.finetune_max_iter = finetune_max_iter\n",
    "        \n",
    "        X = T.dmatrix(name=prefix+'_X')\n",
    "        y = T.lvector(name=prefix+'y')\n",
    "        \n",
    "        # forward propagation through AE layers\n",
    "        self.params = []        \n",
    "        activations = [X]    \n",
    "        for layer in self.ae_layers:\n",
    "            activations += [layer.forward(activations[-1])]\n",
    "            self.params += layer.hidden_layer.params\n",
    "        \n",
    "        # forward progagation through output layer\n",
    "        pred, p_y_given_x = self.softmax_layer.predict(activations[-1])\n",
    "        self.params += softmax_layer.params\n",
    "        \n",
    "        # cost\n",
    "        cost = self.softmax_layer.calc_cost(p_y_given_x, y)\n",
    "        # error\n",
    "        error = 1.0*T.sum(T.neq(pred, y))/X.shape[1]\n",
    "        \n",
    "        grads = T.grad(cost, self.params)\n",
    "        params = [p.type() for p in self.params]\n",
    "        \n",
    "        self.cost_func = theano.function(\n",
    "            inputs=params + [X, y],\n",
    "            outputs=cost,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "        self.grad_func = theano.function(\n",
    "            inputs=params + [X, y],\n",
    "            outputs=grads,\n",
    "            givens=[(s, p) for s, p in zip(self.params, params)])\n",
    "        self.test_model = theano.function(\n",
    "            inputs=[X, y],\n",
    "            outputs=error)\n",
    "        self.run_model = theano.function(\n",
    "            inputs=[X],\n",
    "            outputs=pred)\n",
    "            \n",
    "    def pretrain(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.ndarray, each column stores a sample\n",
    "        \"\"\"\n",
    "        X_in = X\n",
    "        for layer in self.ae_layers:\n",
    "            layer.fit(X_in)\n",
    "            X_out = layer.transform(X_in)\n",
    "            X_in = X_out\n",
    "            \n",
    "        self.softmax_layer.fit(X_in, y)\n",
    "    \n",
    "    def finetune(self, X, y):\n",
    "        init_theta, shapes = pack(map(lambda p: p.get_value(), self.params))\n",
    "        opt_theta, opt_cost, d = sp.optimize.fmin_l_bfgs_b(\n",
    "            func=self.cost_and_grad,\n",
    "            x0=init_theta,\n",
    "            fprime=None,\n",
    "            args=(shapes, X, y),\n",
    "            maxiter=self.finetune_max_iter,\n",
    "            iprint=1)\n",
    "        opt_param_list = unpack(opt_theta, shapes)\n",
    "        for shared_param, opt_param in zip(self.params, opt_param_list):\n",
    "             shared_param.set_value(opt_param)\n",
    "                 \n",
    "    def fit(self, X, y):\n",
    "        self.pretrain(X, y)\n",
    "        self.finetune(X, y)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.run_model(X)\n",
    "  \n",
    "    def evaluate(self, X, y):\n",
    "        return self.test_model(X, y)\n",
    "\n",
    "    def cost_and_grad(self, theta, *args):\n",
    "        \"\"\"\n",
    "        compute the cost and gradient at theta in the parameter space\n",
    "        args should be (shapes, X, y)\n",
    "        X: each column stores a sample\n",
    "        \"\"\"\n",
    "        shapes = args[0]\n",
    "        X = args[1]\n",
    "        y = args[2]\n",
    "        param_list = unpack(theta, shapes)\n",
    "        cost =  self.cost_func(*(param_list+[X, y]))\n",
    "        grad = self.grad_func(*(param_list+[X, y]))\n",
    "        grad, _ = pack(grad)\n",
    "        return cost, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the deep network for digit classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    f = gzip.open('data/mnist.pkl.gz')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    \n",
    "    # train model\n",
    "    train_X = train_set[0].transpose()\n",
    "    train_y = train_set[1]\n",
    "    valid_X = valid_set[0].transpose()\n",
    "    valid_y = valid_set[1]\n",
    "    \n",
    "    train_X = np.hstack((train_X, valid_X))\n",
    "    train_y = np.hstack((train_y, valid_y))\n",
    "    \n",
    "    train_X = train_X[:, 0:60001:5]\n",
    "    train_y = train_y[0:60001:5]\n",
    "    \n",
    "    n_in = train_X.shape[0]\n",
    "    n_out = np.unique(train_y).shape[0]\n",
    "    print \"{n} training samples of dim {d}\".format(n=train_X.shape[1], d=n_in)\n",
    "    \n",
    "    auto_encoder1 = AutoEncoder(\n",
    "        n_in=train_X.shape[0],\n",
    "        n_hid=200,\n",
    "        L2_reg_coef=3e-3,\n",
    "        sparse_reg_coef=3.0,\n",
    "        sparse_rho=0.1,\n",
    "        max_iter=200)  \n",
    "    auto_encoder2 = AutoEncoder(\n",
    "        n_in=auto_encoder1.n_hid,\n",
    "        n_hid = 200,\n",
    "        L2_reg_coef=3e-3,\n",
    "        sparse_reg_coef=3.0,\n",
    "        sparse_rho=0.1,\n",
    "        max_iter=200) \n",
    "    softmax_regression = SoftmaxRegression(\n",
    "        n_in=auto_encoder2.n_hid, \n",
    "        n_out=10, \n",
    "        L2_reg_coef=3e-3, \n",
    "        max_iter=100)\n",
    "    dnn = DNN((auto_encoder1, auto_encoder2), softmax_regression, finetune_max_iter=100)\n",
    "    dnn.fit(train_X, train_y)\n",
    "    \n",
    "    # test model\n",
    "    test_X = test_set[0].transpose()\n",
    "    test_y = test_set[1]\n",
    "    #pred = dnn.transform(test_X)\n",
    "    #error = 1.0*(pred != test_y).sum()/test_y.shape[0]\n",
    "    error = dnn.evaluate(test_X, test_y)\n",
    "    print 'error rate on test set is {e}%, accuracy is {a}%'.format(e=error*100, a=100-100*error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimental results\n",
    "\n",
    "Training set:\n",
    "* 60,000 handwritten digit images of 0~9,\n",
    "* Selecting a subset of the training samples by picking one for every five samples.\n",
    "\n",
    "Test set:\n",
    "* 10,000 handwritten digit images of 0~9.\n",
    "\n",
    "<table>\n",
    "<tbody>\n",
    "<tr><td>Method</td><td>Accuracy</td></tr>\n",
    "<tr><td>Baseline</td><td>9.8%</td></tr>\n",
    "<tr><td>Softmax Regression</td><td>91.5%</td></tr>\n",
    "<tr><td>DNN (1 AE)(pretraining)</td><td>92.4%</td></tr>\n",
    "<tr><td>DNN (1 AE)(fine-tuning)</td><td>93.83%</td></tr>\n",
    "<tr><td>DNN (1 AE)(pretaining plus fine-tuning)</td><td>95.0%</td></tr>\n",
    "<tr><td>DNN (2 AEs)(pretraining)</td><td>85.9%</td></tr>\n",
    "<tr><td>DNN (2 AEs)(fine-tuning)</td><td>93.77%</td></tr>\n",
    "<tr><td>DNN (2 AEs)(pretaining plus fine-tuning)</td><td>95.9%</td></tr>\n",
    "</table>\n",
    "</tbody>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights into the Behaviours of the Neurons\n",
    "\n",
    "Find input $x$ that maximally activates a specific hidden neuron and has limited $L_2$ norm: \n",
    "$$\n",
    "\\max_{x}a^{(l)}_i(x)\\\\\n",
    "s.t. \\Vert x \\Vert^2 \\le 1\n",
    "$$\n",
    "This problem can be approximated by a non-constrained minimisation problem:\n",
    "$$\n",
    "\\min_{x}\\left[-a^{(l)}_i(x) + \\lambda \\max(\\Vert x \\Vert^2 - 1, 0)\\right]\n",
    "$$\n",
    "where $\\lambda$ is a fairly large number.\n",
    "\n",
    "$\\mathcal{l}(x)=\\max(\\Vert x \\Vert^2 - 1, 0)$ can be considered as a hinge loss. Since the derivative of the hinge loss at $\\Vert x \\Vert^2 = 1$ is non-deterministic, smoothed versions may be preferred for more stable optimisation.\n",
    "\n",
    "#### Hinge loss function\n",
    "\n",
    "$\\mathcal{l}(x)=\\max(\\Vert x \\Vert^2 - 1, 0)$\n",
    "\n",
    "As the optimal solution may very well lie in the neighborhood of $\\Vert x \\Vert^2 = 1$, the loss gradient therein should become more \"cautious\" as it approaches the border to avoid bouncing back and forth in vain.\n",
    "\n",
    "#### Smoothed hinge loss function\n",
    "\n",
    "$\\mathcal{l}_s(x)=\\frac{1}{2\\gamma}\\max(\\Vert x \\Vert^2 - 1, 0)^2$\n",
    "\n",
    "![image](hinge_loss_plot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c58f0bcb1cd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#for i in np.random.randint(0, dnn.ae_layers[0].n_hid, 64):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdvector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "# visulisation the first hidden layer\n",
    "plt.close('all')\n",
    "def rescale(x):\n",
    "    # Remove DC (mean of images)\n",
    "    x = x - x.mean()\n",
    "    # Truncate to +/-3 standard deviations and scale to -1 to 1\n",
    "    pstd = 3 * x.std()\n",
    "    x = np.maximum(np.minimum(x, pstd), -pstd) / pstd\n",
    "    # Rescale from [-1,1] to [0.1,0.9]\n",
    "    x = (x + 1) * 0.4 + 0.1;\n",
    "    return x\n",
    "fig = plt.figure(0)\n",
    "k = 0\n",
    "for i in range(0, 64):#for i in np.random.randint(0, dnn.ae_layers[0].n_hid, 64):\n",
    "    x = T.dvector('x')\n",
    "    x0 = np.random.rand(28*28)\n",
    "    x0 = x0 / np.sqrt((x0**2).sum())\n",
    "    a = dnn.ae_layers[0].forward(x)\n",
    "    # cost\n",
    "    cost = -a[i] + 1e2 * T.max([0, (x**2).sum()-1])**2\n",
    "    grad = T.grad(cost, x)\n",
    "    cost_func = theano.function([x], cost)\n",
    "    grad = T.grad(cost, x)\n",
    "    grad_func = theano.function([x], grad)\n",
    "    act_func = theano.function([x], a)\n",
    "    def cost_and_grad(x):\n",
    "        return cost_func(x), grad_func(x)\n",
    "    opt_x, opt_cost, d = sp.optimize.fmin_l_bfgs_b(\n",
    "        func=cost_and_grad,\n",
    "        x0=x0,\n",
    "        maxiter=1000,\n",
    "        iprint=0)\n",
    "    print (\"||x||^2 is {v:.2f}, activation is {a:.2f}\"\n",
    "            .format(v=(opt_x**2).sum(), a=act_func(opt_x)[i]))\n",
    "    opt_x_rescaled = rescale(opt_x)\n",
    "    img = opt_x_rescaled.reshape((28,28))\n",
    "    k = k + 1\n",
    "    ax = fig.add_subplot(8, 8, k)\n",
    "    ax.imshow(img, cmap='gray', interpolation='none')\n",
    "    ax.set_axis_off()\n",
    "plt.show()\n",
    "fig.savefig('visual_dnn_layer1.png', format='png')\n",
    "\n",
    "# visulisation the second hidden layer\n",
    "fig = plt.figure(1)\n",
    "k = 0\n",
    "for i in range(0, 64):#np.random.randint(0, dnn.ae_layers[1].n_hid, 64):\n",
    "    x = T.dvector('x')\n",
    "    x0 = np.random.rand(28*28)\n",
    "    x0 = x0 / np.sqrt((x0**2).sum())\n",
    "    a1 = dnn.ae_layers[0].forward(x)\n",
    "    a = dnn.ae_layers[1].forward(a1)\n",
    "    # cost\n",
    "    cost = -a[i] + 1e2 * T.max([0, (x**2).sum()-1])**2\n",
    "    grad = T.grad(cost, x)\n",
    "    cost_func = theano.function([x], cost)\n",
    "    grad = T.grad(cost, x)\n",
    "    grad_func = theano.function([x], grad)\n",
    "    act_func = theano.function([x], a)\n",
    "    def cost_and_grad(x):\n",
    "        return cost_func(x), grad_func(x)\n",
    "    opt_x, opt_cost, d = sp.optimize.fmin_l_bfgs_b(\n",
    "        func=cost_and_grad,\n",
    "        x0=x0,\n",
    "        maxiter=1000,\n",
    "        iprint=0)\n",
    "    #print opt_x\n",
    "    print (\"||x||^2 is {v:.2f}, activation is {a:.2f}\"\n",
    "            .format(v=(opt_x**2).sum(), a=act_func(opt_x)[i]))        \n",
    "    opt_x_rescaled = rescale(opt_x)\n",
    "    img = opt_x_rescaled.reshape((28,28))\n",
    "    k = k + 1\n",
    "    ax = fig.add_subplot(8, 8, k)\n",
    "    ax.imshow(img, cmap='gray', interpolation='none')\n",
    "    ax.set_axis_off()\n",
    "plt.show()\n",
    "fig.savefig('visual_dnn_layer2.png', format='png')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs that maximise the activation of the first 64 neursons in the first hidden layer after pretraining:\n",
    "![image](visual_dnn_layer1_pretrain.png)\n",
    "\n",
    "Inputs that maximise the activation of of the first 64 neursons in the second hidden layer after pretraining:\n",
    "![image](visual_dnn_layer2_pretrain.png)\n",
    "\n",
    "Inputs that maximise the activation of the first 64 neursons in the first hidden layer after fine-tuning:\n",
    "![image](visual_dnn_layer1_finetune.png)\n",
    "\n",
    "Inputs that maximise the activation of of the first 64 neursons in the second hidden layer after fine-tuning:\n",
    "![image](visual_dnn_layer2_finetune.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generative Model v.s. Discriminative Model\n",
    "* Generative Model\n",
    "    * Tries to model the distribution of patterns in the feature space\n",
    "    * More explainable\n",
    "* Discriminative Model\n",
    "    * Tries to model the differences among patterns\n",
    "    * Less explainable\n",
    "\n",
    "An example is to recognise two handwritten digits $1$ and $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
